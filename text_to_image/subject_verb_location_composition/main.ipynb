{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.append('./../')\n",
    "sys.path.append('./../tirg/')\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17919 745\n",
      "sucessfully loaded features\n",
      "sucessfully loaded features\n",
      "sucessfully loaded features\n"
     ]
    }
   ],
   "source": [
    "opt = parse_opt() \n",
    "opt.batch_size = 32\n",
    "opt.coco_path = '../../../../datasets/coco'\n",
    "opt.sic112_path = '../../../../datasets/SIC112/'\n",
    "\n",
    "logger = SummaryWriter(comment = opt.comment)\n",
    "\n",
    "trainset, _, sic112 = load_datasets(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add subject, verb and location annotations to SIC112\n",
    "for img in sic112.imgs:\n",
    "    img['subjects'] = [img['captions'][0].split()[0]]\n",
    "    if img['captions'][0].split()[1].endswith(\"ing\"):\n",
    "        img['verbs'] = [img['captions'][0].split()[1]]\n",
    "        img['locations'] = [' '.join(img['captions'][0].split()[2:])]\n",
    "    else:\n",
    "        img['verbs'] = []\n",
    "        img['locations'] = [' '.join(img['captions'][0].split()[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:01<00:00, 290800.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# add subject, verb and location annotations to coco train 2014\n",
    "# (need 'coco_splitted_captions_train2014.json' preprocess_coco first)\n",
    "id2img = {}\n",
    "for img in trainset.imgs:\n",
    "    id2img[img['id']] = img\n",
    "    img['subjects'] = []\n",
    "    img['verbs'] = []\n",
    "    img['locations'] = []\n",
    "for caption in tqdm(json.load(open('coco_splitted_captions_train2014.json', 'rt'))['annotations']):\n",
    "    img = id2img[caption['image_id']]\n",
    "    if caption['subject_phrase'] is not None:\n",
    "        img['subjects'] += [caption['subject_phrase']]\n",
    "    if caption['verb_phrase'] is not None:\n",
    "        img['verbs'] += [caption['verb_phrase']]\n",
    "    if caption['location_phrase'] is not None:\n",
    "        img['locations'] += [caption['location_phrase']]\n",
    "        \n",
    "# update trainset.__getitem__\n",
    "#trainset.old_get = trainset.__getitem__\n",
    "#def new_get(self, idx):\n",
    "#    item = self.old_get(idx)\n",
    "#type(trainset).__getitem__ = type(type(trainset).__getitem__)(new_get, trainset, type(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One2OneTransformation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(One2OneTransformation, self).__init__()\n",
    "        embed_dim = opt.embed_dim\n",
    "        self.m = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_dim * 1, embed_dim * 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 2, embed_dim * 2),\n",
    "            torch.nn.BatchNorm1d(embed_dim * 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 2, embed_dim)\n",
    "        )\n",
    "        self.norm = torch_functions.NormalizationLayer(learn_scale=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.norm(x)\n",
    "        f = self.m(f)\n",
    "        return f\n",
    "    \n",
    "class Three2OneTransformation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Three2OneTransformation, self).__init__()\n",
    "        embed_dim = opt.embed_dim\n",
    "        self.m = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_dim * 3, embed_dim * 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 5, embed_dim * 5),\n",
    "            torch.nn.BatchNorm1d(embed_dim * 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 5, embed_dim)\n",
    "        )\n",
    "        self.norm = torch_functions.NormalizationLayer(learn_scale=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = torch.cat([self.norm(i) for i in x], dim=1)\n",
    "        f = self.m(f)\n",
    "        return f\n",
    "\n",
    "model = create_model(opt, trainset)\n",
    "model.subject_extractor = One2OneTransformation()\n",
    "model.verb_extractor = One2OneTransformation()\n",
    "model.location_extractor = One2OneTransformation()\n",
    "model.svl_combine = Three2OneTransformation() \n",
    "model = model.cuda()\n",
    "optimizer = create_optimizer(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_svl(model, testset, opt):\n",
    "    model = model.eval()\n",
    "\n",
    "    # all img features\n",
    "    img_features = []\n",
    "    for data in testset.get_loader(batch_size = opt.batch_size, shuffle = False, drop_last= False):\n",
    "        # extract image features\n",
    "        imgs = np.stack([d['image'] for d in data])\n",
    "        imgs = torch.from_numpy(imgs).float()\n",
    "        if len(imgs.shape) == 2:\n",
    "            imgs = model.img_encoder.fc(imgs.cuda())\n",
    "        else:\n",
    "            imgs = model.img_encoder(imgs.cuda())\n",
    "        imgs = model.snorm(imgs).cpu().detach().numpy()\n",
    "        img_features += [imgs]\n",
    "\n",
    "    img_features = np.concatenate(img_features, axis=0)\n",
    "    img_labels = [img['captions'][0] for img in testset.imgs]\n",
    "    \n",
    "    # construct random queries\n",
    "    queries = []\n",
    "    np.random.seed(123)\n",
    "    for _ in range(5):\n",
    "      for img in testset.imgs:\n",
    "        if len(img['verbs']) == 0:\n",
    "            continue\n",
    "        while True:\n",
    "            i = np.random.randint(0, len(testset.imgs))\n",
    "            if img['subjects'][0] == testset.imgs[i]['subjects'][0] and img is not testset.imgs[i]:\n",
    "                break\n",
    "        while True:\n",
    "            j = np.random.randint(0, len(testset.imgs))\n",
    "            if len(testset.imgs[j]['verbs']) == 0:\n",
    "                continue\n",
    "            if img['verbs'][0] == testset.imgs[j]['verbs'][0] and img is not testset.imgs[j]:\n",
    "                break\n",
    "        while True:\n",
    "            k = np.random.randint(0, len(testset.imgs))\n",
    "            if img['locations'][0] == testset.imgs[k]['locations'][0] and img is not testset.imgs[k]:\n",
    "                break\n",
    "            \n",
    "        \n",
    "        queries += [{\n",
    "            'subject_img_id': i,\n",
    "            'verb_img_id': j,\n",
    "            'location_img_id': k,\n",
    "            'subject': img['subjects'][0],\n",
    "            'verb': testset.imgs[j]['verbs'][0],\n",
    "            'location': img['locations'][0],\n",
    "            'label': img['captions'][0]\n",
    "        }]\n",
    "        \n",
    "    #----\n",
    "    #----\n",
    "    r = []\n",
    "    query_setting_combinations = []\n",
    "    for s in ['t', 'i']:\n",
    "        for v in ['t', 'i']:\n",
    "            for l in ['t', 'i']:\n",
    "                query_setting_combinations += [(s, v, l)]\n",
    "    for s, v, l in query_setting_combinations:\n",
    "        # compute query features\n",
    "        query_features = []\n",
    "        query_labels = []\n",
    "        for i in range(0, len(queries), opt.batch_size):\n",
    "            if s == 'i':\n",
    "                subjects = model.subject_extractor(torch.from_numpy(\n",
    "                    img_features[[q['subject_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                ).cuda())\n",
    "            else:\n",
    "                subjects = model.text_encoder([q['subject'] for q in queries[i:(i+opt.batch_size)]])\n",
    "            if v == 'i':\n",
    "                verbs = model.verb_extractor(torch.from_numpy(\n",
    "                    img_features[[q['verb_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                ).cuda())\n",
    "            else:\n",
    "                verbs = model.text_encoder([q['verb'] for q in queries[i:(i+opt.batch_size)]])\n",
    "            if l == 'i':\n",
    "                locations = model.location_extractor(torch.from_numpy(\n",
    "                    img_features[[q['location_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                ).cuda())\n",
    "            else:\n",
    "                locations = model.text_encoder([q['location'] for q in queries[i:(i+opt.batch_size)]])\n",
    "            svl = model.svl_combine([subjects, verbs, locations])\n",
    "            svl = svl.cpu().detach().numpy()\n",
    "            query_features += [svl]\n",
    "            query_labels += [q['label'] for q in queries[i:(i+opt.batch_size)]]\n",
    "\n",
    "        query_features = np.concatenate(query_features, axis=0)\n",
    "\n",
    "        # compute recall\n",
    "        def measure_retrieval_performance(query_features, name = 'X'):\n",
    "            sims = query_features.dot(img_features.T)\n",
    "            sims = sims\n",
    "            for k in [1, 5, 10]:\n",
    "                r1 = 0.0\n",
    "                r1_novel = 0.0\n",
    "                count_novel = 0.0\n",
    "                r1_nonnovel = 0.0\n",
    "                count_nonnovel = 0.0\n",
    "                for i in range(sims.shape[0]):\n",
    "                    novel_query = False\n",
    "                    if queries[i]['label'].split()[0] in ['trex', 'stormtrooper', 'darthvader', 'chewbacca']:\n",
    "                        novel_query = True\n",
    "                    if novel_query:\n",
    "                        count_novel += 1\n",
    "                    else:\n",
    "                        count_nonnovel += 1\n",
    "                        \n",
    "                    s = -sims[i,:]\n",
    "                    s = np.argsort(s)\n",
    "                    if query_labels[i] in [img_labels[s[j]] for j in range(k)]:\n",
    "                        r1 += 1\n",
    "                        if novel_query:\n",
    "                            r1_novel += 1\n",
    "                        else:\n",
    "                            r1_nonnovel += 1\n",
    "                        \n",
    "                r1 /= sims.shape[0]\n",
    "                r.append(('svl_' + name + '_recall_top' + str(k), r1))\n",
    "            return r\n",
    "        measure_retrieval_performance(query_features, name = s + v + l)\n",
    "    return r\n",
    "\n",
    "def test(model, testset, opt):\n",
    "    n = 1100\n",
    "    if len(testset) < 10000:\n",
    "        n = len(testset)\n",
    "    r = test_text_to_image_retrieval(model, testset, opt, n)\n",
    "    if '112' in testset.name():\n",
    "        r += test_svl(model, testset, opt)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(model, data, losses_tracking, add_extract_compose_losses = True):\n",
    "    losses = []\n",
    "\n",
    "    # joint embedding loss\n",
    "    imgs = np.stack([d['image'] for d in data])\n",
    "    imgs = torch.from_numpy(imgs).float()\n",
    "    if len(imgs.shape) == 2:\n",
    "        imgs = model.img_encoder.fc(imgs.cuda())\n",
    "    else:\n",
    "         imgs = model.img_encoder(imgs.cuda())\n",
    "    texts = [random.choice(d['captions']) for d in data]\n",
    "    texts = model.text_encoder(texts)\n",
    "    loss_name = 'joint_embedding'\n",
    "    loss_weight = 1\n",
    "    loss_value = model.pair_loss(texts, imgs).cuda()\n",
    "    losses += [(loss_name, loss_weight, loss_value)]\n",
    "    \n",
    "    def do_add_extract_compose_losses():\n",
    "        try:\n",
    "            subjects = [random.choice(trainset.imgs[d['index']]['subjects']) for d in data]\n",
    "            verbs = [random.choice(trainset.imgs[d['index']]['verbs']) for d in data]\n",
    "            locations = [random.choice(trainset.imgs[d['index']]['locations']) for d in data]\n",
    "        except:\n",
    "            return\n",
    "        encoded_subjects = model.text_encoder(subjects).detach()\n",
    "        encoded_verbs = model.text_encoder(verbs).detach()\n",
    "        encoded_locations = model.text_encoder(locations).detach()\n",
    "        extracted_subjects = model.subject_extractor(random.choice([texts, imgs]).detach())\n",
    "        extracted_verbs = model.verb_extractor(random.choice([texts, imgs]).detach())\n",
    "        extracted_location = model.location_extractor(random.choice([texts, imgs]).detach())\n",
    "            \n",
    "        # extract\n",
    "        loss_value = 0\n",
    "        loss_value += model.pair_loss(\n",
    "            torch.cat([extracted_subjects, extracted_verbs, extracted_location]),\n",
    "            torch.cat([encoded_subjects, encoded_verbs, encoded_locations])\n",
    "        ).cuda()\n",
    "        loss_name = 'extract'\n",
    "        loss_weight = 1\n",
    "        losses.append((loss_name, loss_weight, loss_value))\n",
    "        \n",
    "        # compose with encoded\n",
    "        loss_value = model.pair_loss(\n",
    "            model.svl_combine([encoded_subjects, encoded_verbs, encoded_locations]),\n",
    "            random.choice([imgs, model.text_encoder([s + ' ' + v + ' ' + l for s, v, l in zip(subjects, verbs, locations)])]).detach()\n",
    "        ).cuda()\n",
    "        loss_name = 'compose1'\n",
    "        loss_weight = 0.5\n",
    "        losses.append((loss_name, loss_weight, loss_value))\n",
    "\n",
    "        # shuffle\n",
    "        shuffled_subjects_indices = range(len(data))\n",
    "        shuffled_verbs_indices = range(len(data))\n",
    "        shuffled_locations_indices = range(len(data))\n",
    "        random.shuffle(shuffled_subjects_indices)\n",
    "        random.shuffle(shuffled_verbs_indices)\n",
    "        random.shuffle(shuffled_locations_indices)\n",
    "        encoded_subjects = encoded_subjects[shuffled_subjects_indices,:]\n",
    "        encoded_verbs = encoded_verbs[shuffled_verbs_indices,:]\n",
    "        encoded_locations = encoded_locations[shuffled_locations_indices,:]\n",
    "        extracted_subjects = extracted_subjects[shuffled_subjects_indices,:]\n",
    "        extracted_verbs = extracted_verbs[shuffled_verbs_indices,:]\n",
    "        extracted_location = extracted_location[shuffled_locations_indices,:]\n",
    "        subjects = np.array(subjects)[shuffled_subjects_indices]\n",
    "        verbs = np.array(verbs)[shuffled_verbs_indices]\n",
    "        locations = np.array(locations)[shuffled_locations_indices]\n",
    "\n",
    "        # compose with extracted\n",
    "        loss_value = model.pair_loss(\n",
    "            model.svl_combine([extracted_subjects, extracted_verbs, extracted_location]),\n",
    "            model.text_encoder([s + ' ' + v + ' ' + l for s, v, l in zip(subjects, verbs, locations)]).detach()\n",
    "        ).cuda()\n",
    "        loss_name = 'compose2'\n",
    "        loss_weight = 0.5\n",
    "        losses.append((loss_name, loss_weight, loss_value))\n",
    "    if add_extract_compose_losses:\n",
    "        do_add_extract_compose_losses()\n",
    "\n",
    "    # total loss\n",
    "    total_loss = sum([loss_weight * loss_value for loss_name, loss_weight, loss_value in losses])\n",
    "    assert(not torch.isnan(total_loss))\n",
    "    losses += [('total training loss', None, total_loss)]\n",
    "\n",
    "    # save losses\n",
    "    for loss_name, loss_weight, loss_value in losses:\n",
    "        if not losses_tracking.has_key(loss_name):\n",
    "            losses_tracking[loss_name] = []\n",
    "        losses_tracking[loss_name].append(float(loss_value.data.item()))\n",
    "    return total_loss\n",
    "\n",
    "def train_1_epoch(model, optimizer, trainset, opt, losses_tracking, add_extract_compose_losses = True):\n",
    "    model.train()\n",
    "    loader = trainset.get_loader(\n",
    "        batch_size=opt.batch_size, shuffle=True,\n",
    "        drop_last=True, num_workers=opt.loader_num_workers)\n",
    "    for data in tqdm(loader, desc = 'training 1 epoch'):\n",
    "        total_loss = compute_losses(model, data, losses_tracking, add_extract_compose_losses)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Elapsed time 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1289\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.1578\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.0368\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.0614\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.0368\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.0737\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.0246\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.0614\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.0268\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.0737\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.0246\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.0562\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.0246\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.0614\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.0123\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.0614\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.0246\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.0614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:53<00:00, 14.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Elapsed time 209.0682\n",
      "    total training loss 3.1741\n",
      "    extract 2.7595\n",
      "    joint_embedding 0.6088\n",
      "    compose2 1.453\n",
      "    compose1 0.4708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1662\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2157\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2038\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.577\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.822\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1875\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.4839\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.6373\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1905\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5096\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.6718\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1405\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.3865\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.5351\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.1729\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.5131\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.6877\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1187\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.3751\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.5419\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1315\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4001\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.5623\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.092\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.2955\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.4325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:52<00:00, 14.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Elapsed time 208.6189\n",
      "    total training loss 3.0037\n",
      "    extract 2.6726\n",
      "    joint_embedding 0.5718\n",
      "    compose2 1.2877\n",
      "    compose1 0.4675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1886\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3297\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2904\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.55\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.7986\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.192\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5622\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7238\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1732\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.4896\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.6731\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1294\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4031\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.5627\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2043\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.5768\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7657\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1517\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.4293\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.589\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1502\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.451\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6155\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1017\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3244\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.4738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:53<00:00, 14.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Elapsed time 209.4395\n",
      "    total training loss 3.0934\n",
      "    extract 2.6032\n",
      "    joint_embedding 0.524\n",
      "    compose2 1.2618\n",
      "    compose1 0.414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2004\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.1691\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2106\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.4899\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.7066\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.172\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5272\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.6998\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1461\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.4684\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.6397\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1395\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4335\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.5982\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.174\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.4765\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.6579\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1441\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.4465\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6152\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1327\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4061\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.572\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1141\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3725\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:53<00:00, 14.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Elapsed time 209.5628\n",
      "    total training loss 2.8595\n",
      "    extract 2.554\n",
      "    joint_embedding 0.4984\n",
      "    compose2 1.1957\n",
      "    compose1 0.3705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.206\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2633\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2406\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.531\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.7041\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1734\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.4758\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.62\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1831\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5413\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.713\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1366\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4114\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.5644\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.201\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.5763\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7466\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1721\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.4618\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6047\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.158\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4658\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6433\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1196\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3774\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:53<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Elapsed time 209.7755\n",
      "    total training loss 2.6107\n",
      "    extract 2.5112\n",
      "    joint_embedding 0.47\n",
      "    compose2 1.1457\n",
      "    compose1 0.3663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2217\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2946\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2775\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.6532\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.7772\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1961\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5599\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7271\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2201\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5542\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.7139\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1375\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.3951\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.5612\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2474\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6295\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.8009\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1767\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.4868\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6676\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1853\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4986\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6678\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1046\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3482\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.4932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Elapsed time 210.0035\n",
      "    total training loss 2.734\n",
      "    extract 2.4719\n",
      "    joint_embedding 0.4289\n",
      "    compose2 1.1028\n",
      "    compose1 0.3556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2442\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3893\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2505\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.5936\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.8478\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2091\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5288\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.677\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2131\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5597\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.719\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1467\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4354\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.5926\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2123\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6304\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.794\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1832\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5037\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6712\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1644\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4879\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6519\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1181\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3761\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:53<00:00, 14.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Elapsed time 209.4456\n",
      "    total training loss 2.4256\n",
      "    extract 2.4631\n",
      "    joint_embedding 0.4171\n",
      "    compose2 1.0581\n",
      "    compose1 0.3422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2442\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2628\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3413\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.7452\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.9091\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2451\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.6638\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.8194\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2359\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5799\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.7382\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1526\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4511\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.6\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2033\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6297\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7831\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1867\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5181\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6794\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1665\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4948\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6685\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1198\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3713\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Elapsed time 210.0769\n",
      "    total training loss 2.3714\n",
      "    extract 2.4432\n",
      "    joint_embedding 0.4277\n",
      "    compose2 1.066\n",
      "    compose1 0.3157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2488\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2633\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2603\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.6955\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.8177\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2708\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.6172\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7788\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.243\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5888\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.7521\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.2031\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.5197\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.685\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2099\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.5871\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7641\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1928\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5206\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6866\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1699\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.5069\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6665\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1546\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.4345\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:53<00:00, 14.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Elapsed time 209.0978\n",
      "    total training loss 2.4619\n",
      "    extract 2.4193\n",
      "    joint_embedding 0.4006\n",
      "    compose2 1.0369\n",
      "    compose1 0.3297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2541\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3405\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2449\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.6808\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.8496\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.24\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.6176\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.764\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2185\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5605\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.7045\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.175\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4831\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.6349\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2296\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6053\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7624\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2037\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5236\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6771\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1799\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.481\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6269\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.136\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3972\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:53<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Elapsed time 209.565\n",
      "    total training loss 2.4806\n",
      "    extract 2.389\n",
      "    joint_embedding 0.4055\n",
      "    compose2 0.982\n",
      "    compose1 0.3106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.269\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3292\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2971\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.7262\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.8895\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2435\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.6002\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7608\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2316\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5778\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.7272\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1682\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4593\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.6097\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2866\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6686\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.8108\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2016\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5385\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.7001\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1812\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4924\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6374\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1358\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.394\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Elapsed time 210.0538\n",
      "    total training loss 2.2738\n",
      "    extract 2.4088\n",
      "    joint_embedding 0.4079\n",
      "    compose2 1.0094\n",
      "    compose1 0.3215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2565\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2832\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2081\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.5709\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.8748\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1763\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.4904\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.6658\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2004\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5705\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.7402\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1713\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4641\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.6158\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2459\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6367\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7896\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2058\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5301\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6942\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1876\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4967\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6557\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1554\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.4269\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Elapsed time 210.0805\n",
      "    total training loss 2.4414\n",
      "    extract 2.3864\n",
      "    joint_embedding 0.3963\n",
      "    compose2 0.9924\n",
      "    compose1 0.2753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2709\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2764\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3112\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.6771\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.8441\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2352\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5886\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7462\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.234\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5937\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.7505\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1718\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4829\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.6414\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2657\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6368\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.8009\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.192\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.517\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6681\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1942\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.5209\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6808\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1371\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.402\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Elapsed time 210.3731\n",
      "    total training loss 2.4901\n",
      "    extract 2.3692\n",
      "    joint_embedding 0.3778\n",
      "    compose2 0.9787\n",
      "    compose1 0.2924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2599\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2571\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2284\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.7624\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.8674\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2087\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5627\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7289\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2394\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.6178\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.7683\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1853\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.5045\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.6587\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2495\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6452\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.8028\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1942\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5239\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6869\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1925\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.5168\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6726\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1439\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.4087\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Elapsed time 211.3697\n",
      "    total training loss 2.4288\n",
      "    extract 2.3744\n",
      "    joint_embedding 0.3757\n",
      "    compose2 0.9635\n",
      "    compose1 0.3096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2674\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2452\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2891\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.7858\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.8913\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2287\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5998\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7589\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2258\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5924\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.7484\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1775\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4815\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.6366\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2388\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6066\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7599\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1896\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5046\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6685\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1924\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.5017\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6491\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.143\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.4069\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [03:15<00:00, 14.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Elapsed time 234.5081\n",
      "    total training loss 2.4525\n",
      "    extract 2.3718\n",
      "    joint_embedding 0.3752\n",
      "    compose2 0.9771\n",
      "    compose1 0.298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2682\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3417\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3407\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.6262\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.8386\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2473\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.6103\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7681\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2393\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5743\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.7401\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1769\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4906\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.6421\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2708\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6309\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7756\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2047\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5443\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.7031\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2026\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.5091\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6614\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.152\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.426\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [03:52<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Elapsed time 281.0886\n",
      "    total training loss 2.4128\n",
      "    extract 2.3737\n",
      "    joint_embedding 0.3774\n",
      "    compose2 0.9867\n",
      "    compose1 0.2697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2743\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3241\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2333\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.6998\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.7974\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2022\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5371\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.698\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2275\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5421\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.6954\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1767\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4566\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.6081\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2546\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6066\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7557\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2106\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5375\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6889\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1961\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4913\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6541\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1477\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.4083\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 14.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Elapsed time 211.3846\n",
      "    total training loss 2.4167\n",
      "    extract 2.3539\n",
      "    joint_embedding 0.3611\n",
      "    compose2 0.9492\n",
      "    compose1 0.2573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2829\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3059\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3419\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.7465\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.9036\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2366\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.6182\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7627\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2502\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.6276\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.7804\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1926\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4928\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.65\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2236\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.6044\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7546\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.198\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5213\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6711\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1754\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.5048\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6712\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1549\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.4408\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:  45%|████▌     | 1168/2586 [01:19<01:39, 14.27it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9d21d998e5fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     train_1_epoch(model, optimizer, trainset, opt, losses_tracking,\n\u001b[0;32m---> 28\u001b[0;31m                   add_extract_compose_losses = epoch>=1)\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-641df29517bd>\u001b[0m in \u001b[0;36mtrain_1_epoch\u001b[0;34m(model, optimizer, trainset, opt, losses_tracking, add_extract_compose_losses)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         drop_last=True, num_workers=opt.loader_num_workers)\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'training 1 epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_tracking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_extract_compose_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nam_poke/.local/lib/python2.7/site-packages/tqdm/_tqdm.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nam_poke/.local/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nam_poke/.local/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nam_poke/.local/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/multiprocessing/queues.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nam_poke/.local/lib/python2.7/site-packages/torch/multiprocessing/queue.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(str)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;31m# Doctest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "losses_tracking = {}\n",
    "epoch = 0\n",
    "tic = time.time()\n",
    "while True:\n",
    "\n",
    "    # show stat, training losses\n",
    "    print 'Epoch', epoch, 'Elapsed time', round(time.time() - tic, 4)\n",
    "    tic = time.time()\n",
    "    for loss_name in losses_tracking:\n",
    "        avg_loss = np.mean(losses_tracking[loss_name][-250:])\n",
    "        print '   ', loss_name, round(avg_loss, 4)\n",
    "        logger.add_scalar(loss_name, avg_loss, epoch)\n",
    "\n",
    "    # test\n",
    "    tests = []\n",
    "    for dataset in [trainset, sic112]:\n",
    "        t = test(model, dataset, opt)\n",
    "        tests += [(dataset.name() + ' ' + metric_name, metric_value) for metric_name, metric_value in t]\n",
    "    for metric_name, metric_value in tests:\n",
    "        print ' ', metric_name, round(metric_value, 4)\n",
    "        logger.add_scalar(metric_name, metric_value, epoch)\n",
    "\n",
    "    # train\n",
    "    if epoch >= opt.num_epochs:\n",
    "        break\n",
    "    train_1_epoch(model, optimizer, trainset, opt, losses_tracking,\n",
    "                  add_extract_compose_losses = epoch>=1)\n",
    "    epoch += 1\n",
    "\n",
    "    # learing rate scheduling\n",
    "    if epoch % opt.learning_rate_decay_frequency == 0:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
