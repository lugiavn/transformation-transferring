{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main drive script to run train/test\n",
    "\n",
    "# clone https://github.com/google/tirg in ./..\n",
    "# copy *.npy from ./.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.append('./../')\n",
    "sys.path.append('./../tirg/')\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parse_opt() \n",
    "opt.batch_size = 32\n",
    "opt.coco_path = '../../../datasets/coco'\n",
    "opt.sic112_path = '../../../datasets/SIC112/'\n",
    "\n",
    "logger = SummaryWriter(comment = opt.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17919 745\n",
      "sucessfully loaded features\n",
      "sucessfully loaded features\n",
      "sucessfully loaded features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:01<00:00, 240719.56it/s]\n"
     ]
    }
   ],
   "source": [
    "trainset, _, sic112 = load_datasets(opt)\n",
    "\n",
    "# add subject, verb and location annotations to SIC112\n",
    "for img in sic112.imgs:\n",
    "    img['subjects'] = [img['captions'][0].split()[0]]\n",
    "    if img['captions'][0].split()[1].endswith(\"ing\"):\n",
    "        img['verbs'] = [img['captions'][0].split()[1]]\n",
    "        img['locations'] = [' '.join(img['captions'][0].split()[2:])]\n",
    "    else:\n",
    "        img['verbs'] = []\n",
    "        img['locations'] = [' '.join(img['captions'][0].split()[1:])]\n",
    "\n",
    "# add subject, verb and location annotations to coco train 2014\n",
    "# (need 'coco_splitted_captions_train2014.json' preprocess_coco first)\n",
    "id2img = {}\n",
    "for img in trainset.imgs:\n",
    "    id2img[img['id']] = img\n",
    "    img['subjects'] = []\n",
    "    img['verbs'] = []\n",
    "    img['locations'] = []\n",
    "for caption in tqdm(json.load(open('coco_splitted_captions_train2014.json', 'rt'))['annotations']):\n",
    "    img = id2img[caption['image_id']]\n",
    "    if caption['subject_phrase'] is not None:\n",
    "        img['subjects'] += [caption['subject_phrase']]\n",
    "    if caption['verb_phrase'] is not None:\n",
    "        img['verbs'] += [caption['verb_phrase']]\n",
    "    if caption['location_phrase'] is not None:\n",
    "        img['locations'] += [caption['location_phrase']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One2OneTransformation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(One2OneTransformation, self).__init__()\n",
    "        embed_dim = opt.embed_dim\n",
    "        self.m = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_dim * 1, embed_dim * 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 2, embed_dim * 2),\n",
    "            torch.nn.BatchNorm1d(embed_dim * 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 2, embed_dim)\n",
    "        )\n",
    "        self.norm = torch_functions.NormalizationLayer(learn_scale=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.norm(x)\n",
    "        f = self.m(f)\n",
    "        return f\n",
    "    \n",
    "class Three2OneTransformation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Three2OneTransformation, self).__init__()\n",
    "        embed_dim = opt.embed_dim\n",
    "        self.m = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_dim * 3, embed_dim * 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 5, embed_dim * 5),\n",
    "            torch.nn.BatchNorm1d(embed_dim * 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 5, embed_dim)\n",
    "        )\n",
    "        self.norm = torch_functions.NormalizationLayer(learn_scale=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = torch.cat([self.norm(i) for i in x], dim=1)\n",
    "        f = self.m(f)\n",
    "        return f\n",
    "\n",
    "model, optimizer = create_model(opt, trainset)\n",
    "model.subject_extractor = One2OneTransformation()\n",
    "model.verb_extractor = One2OneTransformation()\n",
    "model.location_extractor = One2OneTransformation()\n",
    "model.svl_combine = Three2OneTransformation() \n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "params = []\n",
    "params.append({'params': [p for p in model.img_encoder.fc.parameters()]})\n",
    "params.append({'params': [p for p in model.img_encoder.parameters()], 'lr': 0.1 * opt.learning_rate})\n",
    "params.append({'params': [p for p in model.text_encoder.parameters()], 'lr': opt.learning_rate})\n",
    "params.append({'params': [p for p in model.transformer.parameters()], 'weight_decay': opt.weight_decay * 0.1})\n",
    "params.append({'params': [p for p in model.parameters()]})\n",
    "\n",
    "# remove dup params (keep the first one)\n",
    "for i1, p1 in enumerate(params):\n",
    "  for i2, p2 in enumerate(params):\n",
    "    if p1 is not p2:\n",
    "      for p11 in p1['params']:\n",
    "        for j, p22 in enumerate(p2['params']):\n",
    "          if p11 is p22:\n",
    "            p2['params'][j] = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=opt.learning_rate,\n",
    "    momentum=opt.momentum,\n",
    "    weight_decay=opt.weight_decay\n",
    ")\n",
    "if opt.optim == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params,\n",
    "        lr=opt.learning_rate,\n",
    "        weight_decay=opt.weight_decay\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_svl(model, testset, opt):\n",
    "    model = model.eval()\n",
    "\n",
    "    # all img features\n",
    "    img_features = []\n",
    "    for data in testset.get_loader(batch_size = opt.batch_size, shuffle = False, drop_last= False):\n",
    "        # extract image features\n",
    "        imgs = np.stack([d['image'] for d in data])\n",
    "        imgs = torch.from_numpy(imgs).float()\n",
    "        if len(imgs.shape) == 2:\n",
    "            imgs = model.img_encoder.fc(imgs.cuda())\n",
    "        else:\n",
    "            imgs = model.img_encoder(imgs.cuda())\n",
    "        imgs = model.snorm(imgs).cpu().detach().numpy()\n",
    "        img_features += [imgs]\n",
    "\n",
    "    img_features = np.concatenate(img_features, axis=0)\n",
    "    img_labels = [img['captions'][0] for img in testset.imgs]\n",
    "    \n",
    "    # construct random queries\n",
    "    queries = []\n",
    "    np.random.seed(123)\n",
    "    for _ in range(5):\n",
    "      for img in testset.imgs:\n",
    "        if len(img['verbs']) == 0:\n",
    "            continue\n",
    "        while True:\n",
    "            i = np.random.randint(0, len(testset.imgs))\n",
    "            if img['subjects'][0] == testset.imgs[i]['subjects'][0] and img is not testset.imgs[i]:\n",
    "                break\n",
    "        while True:\n",
    "            j = np.random.randint(0, len(testset.imgs))\n",
    "            if len(testset.imgs[j]['verbs']) == 0:\n",
    "                continue\n",
    "            if img['verbs'][0] == testset.imgs[j]['verbs'][0] and img is not testset.imgs[j]:\n",
    "                break\n",
    "        while True:\n",
    "            k = np.random.randint(0, len(testset.imgs))\n",
    "            if img['locations'][0] == testset.imgs[k]['locations'][0] and img is not testset.imgs[k]:\n",
    "                break\n",
    "            \n",
    "        \n",
    "        queries += [{\n",
    "            'subject_img_id': i,\n",
    "            'verb_img_id': j,\n",
    "            'location_img_id': k,\n",
    "            'subject': img['subjects'][0],\n",
    "            'verb': testset.imgs[j]['verbs'][0],\n",
    "            'location': img['locations'][0],\n",
    "            'label': img['captions'][0]\n",
    "        }]\n",
    "        \n",
    "    #----\n",
    "    r = []\n",
    "    query_setting_combinations = []\n",
    "    for s in ['t', 'i']:\n",
    "        for v in ['t', 'i']:\n",
    "            for l in ['t', 'i']:\n",
    "                query_setting_combinations += [(s, v, l)]\n",
    "    for s, v, l in query_setting_combinations:\n",
    "        # compute query features\n",
    "        query_features = []\n",
    "        query_labels = []\n",
    "        for i in range(0, len(queries), opt.batch_size):\n",
    "            if s == 'i':\n",
    "                subjects = model.subject_extractor(torch.from_numpy(\n",
    "                    img_features[[q['subject_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                ).cuda())\n",
    "            else:\n",
    "                subjects = model.text_encoder([q['subject'] for q in queries[i:(i+opt.batch_size)]])\n",
    "            if v == 'i':\n",
    "                verbs = model.verb_extractor(torch.from_numpy(\n",
    "                    img_features[[q['verb_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                ).cuda())\n",
    "            else:\n",
    "                verbs = model.text_encoder([q['verb'] for q in queries[i:(i+opt.batch_size)]])\n",
    "            if l == 'i':\n",
    "                locations = model.location_extractor(torch.from_numpy(\n",
    "                    img_features[[q['location_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                ).cuda())\n",
    "            else:\n",
    "                locations = model.text_encoder([q['location'] for q in queries[i:(i+opt.batch_size)]])\n",
    "            svl = model.svl_combine([subjects, verbs, locations])\n",
    "            svl = svl.cpu().detach().numpy()\n",
    "            query_features += [svl]\n",
    "            query_labels += [q['label'] for q in queries[i:(i+opt.batch_size)]]\n",
    "\n",
    "        query_features = np.concatenate(query_features, axis=0)\n",
    "\n",
    "        # compute recall\n",
    "        def measure_retrieval_performance(query_features, name = 'X'):\n",
    "            sims = query_features.dot(img_features.T)\n",
    "            sims = sims\n",
    "            for k in [1, 5, 10]:\n",
    "                r1 = 0.0\n",
    "                r1_novel = 0.0\n",
    "                count_novel = 0.0\n",
    "                r1_nonnovel = 0.0\n",
    "                count_nonnovel = 0.0\n",
    "                for i in range(sims.shape[0]):\n",
    "                    novel_query = False\n",
    "                    if queries[i]['label'].split()[0] in ['trex', 'stormtrooper', 'darthvader', 'chewbacca']:\n",
    "                        novel_query = True\n",
    "                    if novel_query:\n",
    "                        count_novel += 1\n",
    "                    else:\n",
    "                        count_nonnovel += 1\n",
    "                        \n",
    "                    s = -sims[i,:]\n",
    "                    s = np.argsort(s)\n",
    "                    if query_labels[i] in [img_labels[s[j]] for j in range(k)]:\n",
    "                        r1 += 1\n",
    "                        if novel_query:\n",
    "                            r1_novel += 1\n",
    "                        else:\n",
    "                            r1_nonnovel += 1\n",
    "                        \n",
    "                r1 /= sims.shape[0]\n",
    "                r.append(('svl_' + name + '_recall_top' + str(k), r1))\n",
    "            return r\n",
    "        measure_retrieval_performance(query_features, name = s + v + l)\n",
    "    return r\n",
    "\n",
    "def test(model, testset, opt):\n",
    "    r = test_text_to_image_retrieval(model, testset, opt)\n",
    "    if '112' in testset.name():\n",
    "        r += test_svl(model, testset, opt)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(model, data, losses_tracking, add_extract_compose_losses = True):\n",
    "    losses = []\n",
    "\n",
    "    # joint embedding loss\n",
    "    imgs = np.stack([d['image'] for d in data])\n",
    "    imgs = torch.from_numpy(imgs).float()\n",
    "    if len(imgs.shape) == 2:\n",
    "        imgs = model.img_encoder.fc(imgs.cuda())\n",
    "    else:\n",
    "         imgs = model.img_encoder(imgs.cuda())\n",
    "    texts = [random.choice(d['captions']) for d in data]\n",
    "    texts = model.text_encoder(texts)\n",
    "    loss_name = 'joint_embedding'\n",
    "    loss_weight = 1\n",
    "    loss_value = model.pair_loss(texts, imgs).cuda()\n",
    "    losses += [(loss_name, loss_weight, loss_value)]\n",
    "    \n",
    "    def do_add_extract_compose_losses():\n",
    "        try:\n",
    "            subjects = [random.choice(trainset.imgs[d['index']]['subjects']) for d in data]\n",
    "            verbs = [random.choice(trainset.imgs[d['index']]['verbs']) for d in data]\n",
    "            locations = [random.choice(trainset.imgs[d['index']]['locations']) for d in data]\n",
    "        except:\n",
    "            return\n",
    "        encoded_subjects = model.text_encoder(subjects).detach()\n",
    "        encoded_verbs = model.text_encoder(verbs).detach()\n",
    "        encoded_locations = model.text_encoder(locations).detach()\n",
    "        extracted_subjects = model.subject_extractor(random.choice([texts, imgs]).detach())\n",
    "        extracted_verbs = model.verb_extractor(random.choice([texts, imgs]).detach())\n",
    "        extracted_location = model.location_extractor(random.choice([texts, imgs]).detach())\n",
    "            \n",
    "        # extract\n",
    "        loss_value = 0\n",
    "        loss_value += model.pair_loss(\n",
    "            torch.cat([extracted_subjects, extracted_verbs, extracted_location]),\n",
    "            torch.cat([encoded_subjects, encoded_verbs, encoded_locations])\n",
    "        ).cuda()\n",
    "        loss_name = 'extract'\n",
    "        loss_weight = 1\n",
    "        losses.append((loss_name, loss_weight, loss_value))\n",
    "        \n",
    "        # compose with encoded\n",
    "        loss_value = model.pair_loss(\n",
    "            model.svl_combine([encoded_subjects, encoded_verbs, encoded_locations]),\n",
    "            random.choice([imgs, model.text_encoder([s + ' ' + v + ' ' + l for s, v, l in zip(subjects, verbs, locations)])]).detach()\n",
    "        ).cuda()\n",
    "        loss_name = 'compose1'\n",
    "        loss_weight = 0.5\n",
    "        losses.append((loss_name, loss_weight, loss_value))\n",
    "\n",
    "        # shuffle\n",
    "        shuffled_subjects_indices = range(len(data))\n",
    "        shuffled_verbs_indices = range(len(data))\n",
    "        shuffled_locations_indices = range(len(data))\n",
    "        random.shuffle(shuffled_subjects_indices)\n",
    "        random.shuffle(shuffled_verbs_indices)\n",
    "        random.shuffle(shuffled_locations_indices)\n",
    "        encoded_subjects = encoded_subjects[shuffled_subjects_indices,:]\n",
    "        encoded_verbs = encoded_verbs[shuffled_verbs_indices,:]\n",
    "        encoded_locations = encoded_locations[shuffled_locations_indices,:]\n",
    "        extracted_subjects = extracted_subjects[shuffled_subjects_indices,:]\n",
    "        extracted_verbs = extracted_verbs[shuffled_verbs_indices,:]\n",
    "        extracted_location = extracted_location[shuffled_locations_indices,:]\n",
    "        subjects = np.array(subjects)[shuffled_subjects_indices]\n",
    "        verbs = np.array(verbs)[shuffled_verbs_indices]\n",
    "        locations = np.array(locations)[shuffled_locations_indices]\n",
    "\n",
    "        # compose with extracted\n",
    "        loss_value = model.pair_loss(\n",
    "            model.svl_combine([extracted_subjects, extracted_verbs, extracted_location]),\n",
    "            model.text_encoder([s + ' ' + v + ' ' + l for s, v, l in zip(subjects, verbs, locations)]).detach()\n",
    "        ).cuda()\n",
    "        loss_name = 'compose2'\n",
    "        loss_weight = 0.5\n",
    "        losses.append((loss_name, loss_weight, loss_value))\n",
    "    if add_extract_compose_losses:\n",
    "        do_add_extract_compose_losses()\n",
    "\n",
    "    # total loss\n",
    "    total_loss = sum([loss_weight * loss_value for loss_name, loss_weight, loss_value in losses])\n",
    "    assert(not torch.isnan(total_loss))\n",
    "    losses += [('total training loss', None, total_loss)]\n",
    "\n",
    "    # save losses\n",
    "    for loss_name, loss_weight, loss_value in losses:\n",
    "        if not losses_tracking.has_key(loss_name):\n",
    "            losses_tracking[loss_name] = []\n",
    "        losses_tracking[loss_name].append(float(loss_value.data.item()))\n",
    "    return total_loss\n",
    "\n",
    "def train_1_epoch(model, optimizer, trainset, opt, losses_tracking, add_extract_compose_losses = True):\n",
    "    model.train()\n",
    "    loader = trainset.get_loader(\n",
    "        batch_size=opt.batch_size, shuffle=True,\n",
    "        drop_last=True, num_workers=opt.loader_num_workers)\n",
    "    for data in tqdm(loader, desc = 'training 1 epoch'):\n",
    "        total_loss = compute_losses(model, data, losses_tracking, add_extract_compose_losses)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Elapsed time 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.0005\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.0123\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.0467\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.0712\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.0123\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.0459\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.0816\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.0123\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.0341\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.0712\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.0123\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.037\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.0713\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.0123\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.0344\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.0816\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.0123\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.0467\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.0816\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.0123\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.0467\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.0961\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.0123\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.0467\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [01:16<00:00, 35.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Elapsed time 113.1016\n",
      "    total training loss 0.7649\n",
      "    joint_embedding 0.7649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1326\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2045\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.0123\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.0123\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.0021\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.0246\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.0\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.0246\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.0123\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.0237\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.0123\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.0269\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.0246\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.0339\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.0246\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.0246\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.0244\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 14.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Elapsed time 211.9801\n",
      "    total training loss 3.2935\n",
      "    extract 2.7807\n",
      "    joint_embedding 0.6419\n",
      "    compose2 1.4407\n",
      "    compose1 0.4638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1551\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.1893\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2192\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.5224\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.7772\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1684\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5192\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.6916\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1817\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.4927\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.6748\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1324\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.3876\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.5439\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.1714\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.5176\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7014\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1338\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.4277\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.5913\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1389\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4316\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.5999\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1003\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3265\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.4697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:56<00:00, 14.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Elapsed time 211.3612\n",
      "    total training loss 2.9445\n",
      "    extract 2.6656\n",
      "    joint_embedding 0.5478\n",
      "    compose2 1.3172\n",
      "    compose1 0.4592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1959\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.208\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2044\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.5727\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.763\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1686\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5193\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.675\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.191\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.4956\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.662\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1389\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4076\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.5616\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.1899\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.5429\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7131\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1585\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.4419\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6021\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1623\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4745\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6255\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1095\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3468\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.4904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:56<00:00, 14.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Elapsed time 211.9835\n",
      "    total training loss 2.9333\n",
      "    extract 2.6095\n",
      "    joint_embedding 0.5068\n",
      "    compose2 1.2512\n",
      "    compose1 0.4309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2099\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.217\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.143\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.5077\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.6765\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1812\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5223\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.6786\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1786\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5077\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.6953\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1655\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4485\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.6021\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.1824\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.5281\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7041\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1416\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.4291\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.592\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.147\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4506\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6343\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1053\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3538\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:56<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Elapsed time 211.8101\n",
      "    total training loss 2.6842\n",
      "    extract 2.5427\n",
      "    joint_embedding 0.4746\n",
      "    compose2 1.1771\n",
      "    compose1 0.381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2277\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2259\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2056\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.655\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.7993\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1921\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5434\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7087\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2101\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.524\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.6728\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1613\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4481\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.5998\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.1699\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.5087\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.6586\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1595\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.4403\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6063\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1505\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.44\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.5991\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1127\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3595\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:56<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Elapsed time 211.9347\n",
      "    total training loss 2.6923\n",
      "    extract 2.5058\n",
      "    joint_embedding 0.4551\n",
      "    compose2 1.1243\n",
      "    compose1 0.3732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2326\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2634\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2597\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.6986\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.8564\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1964\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.561\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7408\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1834\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5115\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.6855\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1546\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4421\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.6043\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2212\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.5743\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.732\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1811\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.5133\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6707\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1568\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4399\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.5945\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1331\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.388\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:56<00:00, 14.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Elapsed time 212.4345\n",
      "    total training loss 2.6842\n",
      "    extract 2.4965\n",
      "    joint_embedding 0.447\n",
      "    compose2 1.0986\n",
      "    compose1 0.3732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2457\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2589\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2676\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top5 0.6869\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top10 0.8042\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1876\n",
      "  SimpleImageCaptions112 svl_tti_recall_top5 0.5535\n",
      "  SimpleImageCaptions112 svl_tti_recall_top10 0.7187\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.216\n",
      "  SimpleImageCaptions112 svl_tit_recall_top5 0.5535\n",
      "  SimpleImageCaptions112 svl_tit_recall_top10 0.715\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1406\n",
      "  SimpleImageCaptions112 svl_tii_recall_top5 0.4194\n",
      "  SimpleImageCaptions112 svl_tii_recall_top10 0.5915\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2371\n",
      "  SimpleImageCaptions112 svl_itt_recall_top5 0.5918\n",
      "  SimpleImageCaptions112 svl_itt_recall_top10 0.7651\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.16\n",
      "  SimpleImageCaptions112 svl_iti_recall_top5 0.4646\n",
      "  SimpleImageCaptions112 svl_iti_recall_top10 0.6355\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1708\n",
      "  SimpleImageCaptions112 svl_iit_recall_top5 0.4936\n",
      "  SimpleImageCaptions112 svl_iit_recall_top10 0.6577\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1266\n",
      "  SimpleImageCaptions112 svl_iii_recall_top5 0.3777\n",
      "  SimpleImageCaptions112 svl_iii_recall_top10 0.5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:  25%|██▍       | 636/2586 [00:43<02:03, 15.80it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9d21d998e5fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     train_1_epoch(model, optimizer, trainset, opt, losses_tracking,\n\u001b[0;32m---> 28\u001b[0;31m                   add_extract_compose_losses = epoch>=1)\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-641df29517bd>\u001b[0m in \u001b[0;36mtrain_1_epoch\u001b[0;34m(model, optimizer, trainset, opt, losses_tracking, add_extract_compose_losses)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_tracking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_extract_compose_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nam_poke/.local/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nam_poke/.local/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "losses_tracking = {}\n",
    "epoch = 0\n",
    "tic = time.time()\n",
    "while True:\n",
    "\n",
    "    # show stat, training losses\n",
    "    print 'Epoch', epoch, 'Elapsed time', round(time.time() - tic, 4)\n",
    "    tic = time.time()\n",
    "    for loss_name in losses_tracking:\n",
    "        avg_loss = np.mean(losses_tracking[loss_name][-250:])\n",
    "        print '   ', loss_name, round(avg_loss, 4)\n",
    "        logger.add_scalar(loss_name, avg_loss, epoch)\n",
    "\n",
    "    # test\n",
    "    tests = []\n",
    "    for dataset in [trainset, sic112]:\n",
    "        t = test(model, dataset, opt)\n",
    "        tests += [(dataset.name() + ' ' + metric_name, metric_value) for metric_name, metric_value in t]\n",
    "    for metric_name, metric_value in tests:\n",
    "        print ' ', metric_name, round(metric_value, 4)\n",
    "        logger.add_scalar(metric_name, metric_value, epoch)\n",
    "\n",
    "    # train\n",
    "    if epoch >= opt.num_epochs:\n",
    "        break\n",
    "    train_1_epoch(model, optimizer, trainset, opt, losses_tracking,\n",
    "                  add_extract_compose_losses = epoch>=1)\n",
    "    epoch += 1\n",
    "\n",
    "    # learing rate scheduling\n",
    "    if epoch % opt.learning_rate_decay_frequency == 0:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
