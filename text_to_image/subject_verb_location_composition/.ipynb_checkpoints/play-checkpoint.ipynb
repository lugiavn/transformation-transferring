{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.append('./../')\n",
    "sys.path.append('./../tirg/')\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17919 745\n",
      "sucessfully loaded features\n",
      "sucessfully loaded features\n",
      "sucessfully loaded features\n"
     ]
    }
   ],
   "source": [
    "opt = parse_opt() \n",
    "opt.batch_size = 32\n",
    "opt.coco_path = '../../../../datasets/coco'\n",
    "opt.sic112_path = '../../../../datasets/SIC112/'\n",
    "\n",
    "logger = SummaryWriter(comment = opt.comment)\n",
    "\n",
    "trainset, _, sic112 = load_datasets(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add subject, verb and location annotations to SIC112\n",
    "for img in sic112.imgs:\n",
    "    img['subjects'] = [img['captions'][0].split()[0]]\n",
    "    if img['captions'][0].split()[1].endswith(\"ing\"):\n",
    "        img['verbs'] = [img['captions'][0].split()[1]]\n",
    "        img['locations'] = [' '.join(img['captions'][0].split()[2:])]\n",
    "    else:\n",
    "        img['verbs'] = []\n",
    "        img['locations'] = [' '.join(img['captions'][0].split()[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:01<00:00, 250927.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# add subject, verb and location annotations to coco train 2014\n",
    "# (need 'coco_splitted_captions_train2014.json' preprocess_coco first)\n",
    "id2img = {}\n",
    "for img in trainset.imgs:\n",
    "    id2img[img['id']] = img\n",
    "    img['subjects'] = []\n",
    "    img['verbs'] = []\n",
    "    img['locations'] = []\n",
    "for caption in tqdm(json.load(open('coco_splitted_captions_train2014.json', 'rt'))['annotations']):\n",
    "    img = id2img[caption['image_id']]\n",
    "    if caption['subject_phrase'] is not None:\n",
    "        img['subjects'] += [caption['subject_phrase']]\n",
    "    if caption['verb_phrase'] is not None:\n",
    "        img['verbs'] += [caption['verb_phrase']]\n",
    "    if caption['location_phrase'] is not None:\n",
    "        img['locations'] += [caption['location_phrase']]\n",
    "        \n",
    "# update trainset.__getitem__\n",
    "#trainset.old_get = trainset.__getitem__\n",
    "#def new_get(self, idx):\n",
    "#    item = self.old_get(idx)\n",
    "#type(trainset).__getitem__ = type(type(trainset).__getitem__)(new_get, trainset, type(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One2OneTransformation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(One2OneTransformation, self).__init__()\n",
    "        embed_dim = opt.embed_dim\n",
    "        self.m = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_dim * 1, embed_dim * 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 2, embed_dim * 2),\n",
    "            torch.nn.BatchNorm1d(embed_dim * 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 2, embed_dim)\n",
    "        )\n",
    "        self.norm = torch_functions.NormalizationLayer(learn_scale=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.norm(x)\n",
    "        f = self.m(f)\n",
    "        return f\n",
    "    \n",
    "class Three2OneTransformation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Three2OneTransformation, self).__init__()\n",
    "        embed_dim = opt.embed_dim\n",
    "        self.m = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_dim * 3, embed_dim * 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 5, embed_dim * 5),\n",
    "            torch.nn.BatchNorm1d(embed_dim * 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 5, embed_dim)\n",
    "        )\n",
    "        self.norm = torch_functions.NormalizationLayer(learn_scale=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = torch.cat([self.norm(i) for i in x], dim=1)\n",
    "        f = self.m(f)\n",
    "        return f\n",
    "\n",
    "model = create_model(opt, trainset)\n",
    "model.subject_extractor = One2OneTransformation()\n",
    "model.verb_extractor = One2OneTransformation()\n",
    "model.location_extractor = One2OneTransformation()\n",
    "model.svl_combine = Three2OneTransformation() \n",
    "model = model.cuda()\n",
    "optimizer = create_optimizer(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_svl(model, testset, opt):\n",
    "    model = model.eval()\n",
    "\n",
    "    # all img features\n",
    "    img_features = []\n",
    "    for data in testset.get_loader(batch_size = opt.batch_size, shuffle = False, drop_last= False):\n",
    "        # extract image features\n",
    "        imgs = np.stack([d['image'] for d in data])\n",
    "        imgs = torch.from_numpy(imgs).float()\n",
    "        if len(imgs.shape) == 2:\n",
    "            imgs = model.img_encoder.fc(imgs.cuda())\n",
    "        else:\n",
    "            imgs = model.img_encoder(imgs.cuda())\n",
    "        imgs = model.snorm(imgs).cpu().detach().numpy()\n",
    "        img_features += [imgs]\n",
    "\n",
    "    img_features = np.concatenate(img_features, axis=0)\n",
    "    img_labels = [img['captions'][0] for img in testset.imgs]\n",
    "    \n",
    "    # construct random queries\n",
    "    queries = []\n",
    "    np.random.seed(123)\n",
    "    for _ in range(5):\n",
    "      for img in testset.imgs:\n",
    "        if len(img['verbs']) == 0:\n",
    "            continue\n",
    "        while True:\n",
    "            i = np.random.randint(0, len(testset.imgs))\n",
    "            if img['subjects'][0] == testset.imgs[i]['subjects'][0] and img is not testset.imgs[i]:\n",
    "                break\n",
    "        while True:\n",
    "            j = np.random.randint(0, len(testset.imgs))\n",
    "            if len(testset.imgs[j]['verbs']) == 0:\n",
    "                continue\n",
    "            if img['verbs'][0] == testset.imgs[j]['verbs'][0] and img is not testset.imgs[j]:\n",
    "                break\n",
    "        while True:\n",
    "            k = np.random.randint(0, len(testset.imgs))\n",
    "            if img['locations'][0] == testset.imgs[k]['locations'][0] and img is not testset.imgs[k]:\n",
    "                break\n",
    "            \n",
    "        \n",
    "        queries += [{\n",
    "            'subject_img_id': i,\n",
    "            'verb_img_id': j,\n",
    "            'location_img_id': k,\n",
    "            'subject': img['subjects'][0],\n",
    "            'verb': testset.imgs[j]['verbs'][0],\n",
    "            'location': img['locations'][0],\n",
    "            'label': img['captions'][0]\n",
    "        }]\n",
    "        \n",
    "    #----\n",
    "    #----\n",
    "    r = []\n",
    "    query_setting_combinations = []\n",
    "    for s in ['t', 'i']:\n",
    "        for v in ['t', 'i']:\n",
    "            for l in ['t', 'i']:\n",
    "                query_setting_combinations += [(s, v, l)]\n",
    "    for s, v, l in query_setting_combinations:\n",
    "        # compute query features\n",
    "        query_features = []\n",
    "        query_labels = []\n",
    "        for i in range(0, len(queries), opt.batch_size):\n",
    "            if s == 'i':\n",
    "                subjects = model.subject_extractor(torch.from_numpy(\n",
    "                    img_features[[q['subject_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                ).cuda())\n",
    "            else:\n",
    "                subjects = model.text_encoder([q['subject'] for q in queries[i:(i+opt.batch_size)]])\n",
    "            if v == 'i':\n",
    "                verbs = model.verb_extractor(torch.from_numpy(\n",
    "                    img_features[[q['verb_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                ).cuda())\n",
    "            else:\n",
    "                verbs = model.text_encoder([q['verb'] for q in queries[i:(i+opt.batch_size)]])\n",
    "            if l == 'i':\n",
    "                locations = model.location_extractor(torch.from_numpy(\n",
    "                    img_features[[q['location_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                ).cuda())\n",
    "            else:\n",
    "                locations = model.text_encoder([q['location'] for q in queries[i:(i+opt.batch_size)]])\n",
    "            svl = model.svl_combine([subjects, verbs, locations])\n",
    "            svl = svl.cpu().detach().numpy()\n",
    "            query_features += [svl]\n",
    "            query_labels += [q['label'] for q in queries[i:(i+opt.batch_size)]]\n",
    "\n",
    "        query_features = np.concatenate(query_features, axis=0)\n",
    "\n",
    "        # compute recall\n",
    "        def measure_retrieval_performance(query_features, name = 'X'):\n",
    "            sims = query_features.dot(img_features.T)\n",
    "            sims = sims\n",
    "            for k in [1, 5, 10]:\n",
    "                r1 = 0.0\n",
    "                r1_novel = 0.0\n",
    "                count_novel = 0.0\n",
    "                r1_nonnovel = 0.0\n",
    "                count_nonnovel = 0.0\n",
    "                for i in range(sims.shape[0]):\n",
    "                    novel_query = False\n",
    "                    if queries[i]['label'].split()[0] in ['trex', 'stormtrooper', 'darthvader', 'chewbacca']:\n",
    "                        novel_query = True\n",
    "                    if novel_query:\n",
    "                        count_novel += 1\n",
    "                    else:\n",
    "                        count_nonnovel += 1\n",
    "                        \n",
    "                    s = -sims[i,:]\n",
    "                    s = np.argsort(s)\n",
    "                    if query_labels[i] in [img_labels[s[j]] for j in range(k)]:\n",
    "                        r1 += 1\n",
    "                        if novel_query:\n",
    "                            r1_novel += 1\n",
    "                        else:\n",
    "                            r1_nonnovel += 1\n",
    "                        \n",
    "                r1 /= sims.shape[0]\n",
    "                r.append(('svl_' + name + '_recall_top' + str(k), r1))\n",
    "            return r\n",
    "        measure_retrieval_performance(query_features, name = s + v + l)\n",
    "    return r\n",
    "\n",
    "def test(model, testset, opt):\n",
    "    n = 1100\n",
    "    if len(testset) < 10000:\n",
    "        n = len(testset)\n",
    "    r = test_text_to_image_retrieval(model, testset, opt, n)\n",
    "    if '112' in testset.name():\n",
    "        r += test_svl(model, testset, opt)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(model, data, losses_tracking, add_extract_compose_losses = True):\n",
    "    losses = []\n",
    "\n",
    "    # joint embedding loss\n",
    "    imgs = np.stack([d['image'] for d in data])\n",
    "    imgs = torch.from_numpy(imgs).float()\n",
    "    if len(imgs.shape) == 2:\n",
    "        imgs = model.img_encoder.fc(imgs.cuda())\n",
    "    else:\n",
    "         imgs = model.img_encoder(imgs.cuda())\n",
    "    texts = [random.choice(d['captions']) for d in data]\n",
    "    texts = model.text_encoder(texts)\n",
    "    loss_name = 'joint_embedding'\n",
    "    loss_weight = 1\n",
    "    loss_value = model.pair_loss(texts, imgs).cuda()\n",
    "    losses += [(loss_name, loss_weight, loss_value)]\n",
    "    \n",
    "    def do_add_extract_compose_losses():\n",
    "        try:\n",
    "            subjects = [random.choice(trainset.imgs[d['index']]['subjects']) for d in data]\n",
    "            verbs = [random.choice(trainset.imgs[d['index']]['verbs']) for d in data]\n",
    "            locations = [random.choice(trainset.imgs[d['index']]['locations']) for d in data]\n",
    "        except:\n",
    "            return\n",
    "        encoded_subjects = model.text_encoder(subjects).detach()\n",
    "        encoded_verbs = model.text_encoder(verbs).detach()\n",
    "        encoded_locations = model.text_encoder(locations).detach()\n",
    "        extracted_subjects = model.subject_extractor(random.choice([texts, imgs]).detach())\n",
    "        extracted_verbs = model.verb_extractor(random.choice([texts, imgs]).detach())\n",
    "        extracted_location = model.location_extractor(random.choice([texts, imgs]).detach())\n",
    "            \n",
    "        # extract\n",
    "        loss_value = 0\n",
    "        loss_value += model.pair_loss(\n",
    "            torch.cat([extracted_subjects, extracted_verbs, extracted_location]),\n",
    "            torch.cat([encoded_subjects, encoded_verbs, encoded_locations])\n",
    "        ).cuda()\n",
    "        loss_name = 'extract'\n",
    "        loss_weight = 1\n",
    "        losses.append((loss_name, loss_weight, loss_value))\n",
    "        \n",
    "        # compose with encoded\n",
    "        loss_value = model.pair_loss(\n",
    "            model.svl_combine([encoded_subjects, encoded_verbs, encoded_locations]),\n",
    "            random.choice([imgs, model.text_encoder([s + ' ' + v + ' ' + l for s, v, l in zip(subjects, verbs, locations)])]).detach()\n",
    "        ).cuda()\n",
    "        loss_name = 'compose1'\n",
    "        loss_weight = 0.5\n",
    "        losses.append((loss_name, loss_weight, loss_value))\n",
    "\n",
    "        # shuffle\n",
    "        shuffled_subjects_indices = range(len(data))\n",
    "        shuffled_verbs_indices = range(len(data))\n",
    "        shuffled_locations_indices = range(len(data))\n",
    "        random.shuffle(shuffled_subjects_indices)\n",
    "        random.shuffle(shuffled_verbs_indices)\n",
    "        random.shuffle(shuffled_locations_indices)\n",
    "        encoded_subjects = encoded_subjects[shuffled_subjects_indices,:]\n",
    "        encoded_verbs = encoded_verbs[shuffled_verbs_indices,:]\n",
    "        encoded_locations = encoded_locations[shuffled_locations_indices,:]\n",
    "        extracted_subjects = extracted_subjects[shuffled_subjects_indices,:]\n",
    "        extracted_verbs = extracted_verbs[shuffled_verbs_indices,:]\n",
    "        extracted_location = extracted_location[shuffled_locations_indices,:]\n",
    "        subjects = np.array(subjects)[shuffled_subjects_indices]\n",
    "        verbs = np.array(verbs)[shuffled_verbs_indices]\n",
    "        locations = np.array(locations)[shuffled_locations_indices]\n",
    "\n",
    "        # compose with extracted\n",
    "        loss_value = model.pair_loss(\n",
    "            model.svl_combine([extracted_subjects, extracted_verbs, extracted_location]),\n",
    "            model.text_encoder([s + ' ' + v + ' ' + l for s, v, l in zip(subjects, verbs, locations)]).detach()\n",
    "        ).cuda()\n",
    "        loss_name = 'compose2'\n",
    "        loss_weight = 0.5\n",
    "        losses.append((loss_name, loss_weight, loss_value))\n",
    "    if add_extract_compose_losses:\n",
    "        do_add_extract_compose_losses()\n",
    "\n",
    "    # total loss\n",
    "    total_loss = sum([loss_weight * loss_value for loss_name, loss_weight, loss_value in losses])\n",
    "    assert(not torch.isnan(total_loss))\n",
    "    losses += [('total training loss', None, total_loss)]\n",
    "\n",
    "    # save losses\n",
    "    for loss_name, loss_weight, loss_value in losses:\n",
    "        if not losses_tracking.has_key(loss_name):\n",
    "            losses_tracking[loss_name] = []\n",
    "        losses_tracking[loss_name].append(float(loss_value.data.item()))\n",
    "    return total_loss\n",
    "\n",
    "def train_1_epoch(model, optimizer, trainset, opt, losses_tracking, add_extract_compose_losses = True):\n",
    "    model.train()\n",
    "    loader = trainset.get_loader(\n",
    "        batch_size=opt.batch_size, shuffle=True,\n",
    "        drop_last=True, num_workers=opt.loader_num_workers)\n",
    "    for data in tqdm(loader, desc = 'training 1 epoch'):\n",
    "        total_loss = compute_losses(model, data, losses_tracking, add_extract_compose_losses)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Elapsed time 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.0011\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.0227\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.0107\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.0054\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.0072\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.0063\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.0065\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.006\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.0056\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [01:16<00:00, 33.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Elapsed time 108.7479\n",
      "    total training loss 0.7641\n",
      "    joint_embedding 0.7641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1219\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.1844\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.0014\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.0011\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.0\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.0005\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.0011\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.0027\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.0006\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:53<00:00, 14.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Elapsed time 206.0967\n",
      "    total training loss 3.2498\n",
      "    extract 2.775\n",
      "    joint_embedding 0.6279\n",
      "    compose2 1.4563\n",
      "    compose1 0.4688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1565\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.1305\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.1686\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1353\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1343\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1198\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.1371\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1197\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1285\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:53<00:00, 14.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Elapsed time 206.2718\n",
      "    total training loss 3.0355\n",
      "    extract 2.6744\n",
      "    joint_embedding 0.5563\n",
      "    compose2 1.3228\n",
      "    compose1 0.4397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1785\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2452\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2166\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.147\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1961\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1368\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.1952\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1319\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1516\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Elapsed time 207.8454\n",
      "    total training loss 3.0256\n",
      "    extract 2.5992\n",
      "    joint_embedding 0.5099\n",
      "    compose2 1.2393\n",
      "    compose1 0.4318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1922\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.1691\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.1508\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.1526\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.184\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1562\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.1957\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1471\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1538\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Elapsed time 207.3423\n",
      "    total training loss 2.9012\n",
      "    extract 2.5637\n",
      "    joint_embedding 0.4707\n",
      "    compose2 1.2035\n",
      "    compose1 0.3899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.214\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2361\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2242\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2345\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1967\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1789\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.1622\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1665\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.167\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 16.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Elapsed time 208.184\n",
      "    total training loss 2.6914\n",
      "    extract 2.5051\n",
      "    joint_embedding 0.4568\n",
      "    compose2 1.1329\n",
      "    compose1 0.3531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2185\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2316\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2654\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2125\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1994\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1694\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2156\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1801\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1549\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Elapsed time 207.734\n",
      "    total training loss 2.7476\n",
      "    extract 2.5028\n",
      "    joint_embedding 0.4452\n",
      "    compose2 1.1216\n",
      "    compose1 0.3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2265\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2599\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2103\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2048\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.1871\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.149\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2009\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1686\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1558\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Elapsed time 207.6855\n",
      "    total training loss 2.8254\n",
      "    extract 2.4708\n",
      "    joint_embedding 0.426\n",
      "    compose2 1.0893\n",
      "    compose1 0.3241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2399\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2594\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2048\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2005\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.205\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1714\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2009\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1747\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.174\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Elapsed time 207.3661\n",
      "    total training loss 2.4369\n",
      "    extract 2.4388\n",
      "    joint_embedding 0.4047\n",
      "    compose2 1.0423\n",
      "    compose1 0.3112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2442\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2594\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2882\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2281\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2066\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1678\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2253\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1808\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.161\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Elapsed time 207.504\n",
      "    total training loss 2.6795\n",
      "    extract 2.4444\n",
      "    joint_embedding 0.422\n",
      "    compose2 1.0637\n",
      "    compose1 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2429\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.269\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2889\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2211\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.216\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1713\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2343\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1877\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1746\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Elapsed time 207.9085\n",
      "    total training loss 2.7965\n",
      "    extract 2.439\n",
      "    joint_embedding 0.4056\n",
      "    compose2 1.0666\n",
      "    compose1 0.3587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2659\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2645\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2467\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2171\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2065\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1764\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2237\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1813\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1694\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Elapsed time 208.2331\n",
      "    total training loss 2.5605\n",
      "    extract 2.3914\n",
      "    joint_embedding 0.4017\n",
      "    compose2 0.9793\n",
      "    compose1 0.2956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2583\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2418\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2805\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2346\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2074\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1745\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2581\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1946\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1666\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Elapsed time 207.6126\n",
      "    total training loss 2.4485\n",
      "    extract 2.4047\n",
      "    joint_embedding 0.3831\n",
      "    compose2 1.0061\n",
      "    compose1 0.3269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2588\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2656\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2792\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2392\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2171\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1683\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2323\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1999\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1805\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Elapsed time 208.462\n",
      "    total training loss 2.4979\n",
      "    extract 2.3823\n",
      "    joint_embedding 0.3733\n",
      "    compose2 0.9791\n",
      "    compose1 0.3136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2609\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3275\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3001\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2591\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.244\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1833\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2398\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2002\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1796\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Elapsed time 207.9579\n",
      "    total training loss 2.4561\n",
      "    extract 2.3449\n",
      "    joint_embedding 0.3821\n",
      "    compose2 0.9635\n",
      "    compose1 0.2948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2738\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3297\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2465\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2215\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2153\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1696\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2234\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1867\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1637\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 14.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Elapsed time 208.2679\n",
      "    total training loss 2.3487\n",
      "    extract 2.3293\n",
      "    joint_embedding 0.3731\n",
      "    compose2 0.9599\n",
      "    compose1 0.2476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2625\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3246\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3328\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2301\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2471\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1875\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2277\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1837\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1736\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 16.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Elapsed time 207.7347\n",
      "    total training loss 2.4302\n",
      "    extract 2.3416\n",
      "    joint_embedding 0.3659\n",
      "    compose2 0.9705\n",
      "    compose1 0.2793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2757\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3201\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3263\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2478\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2291\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1871\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.225\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1905\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1618\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Elapsed time 207.5964\n",
      "    total training loss 2.5365\n",
      "    extract 2.3359\n",
      "    joint_embedding 0.3673\n",
      "    compose2 0.935\n",
      "    compose1 0.2717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2724\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3365\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2985\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2473\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2322\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1806\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2285\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.1979\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1619\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:  74%|███████▎  | 1902/2586 [02:09<00:48, 13.98it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 13.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Elapsed time 208.4869\n",
      "    total training loss 2.1445\n",
      "    extract 2.244\n",
      "    joint_embedding 0.2822\n",
      "    compose2 0.802\n",
      "    compose1 0.2452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.322\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.328\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.2977\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2635\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.223\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1962\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2447\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2184\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2021\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Elapsed time 207.5677\n",
      "    total training loss 2.1127\n",
      "    extract 2.217\n",
      "    joint_embedding 0.2899\n",
      "    compose2 0.7953\n",
      "    compose1 0.2239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3207\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3309\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3389\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2832\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2362\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1973\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2408\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2306\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1966\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Elapsed time 208.3516\n",
      "    total training loss 2.0324\n",
      "    extract 2.1968\n",
      "    joint_embedding 0.2735\n",
      "    compose2 0.7422\n",
      "    compose1 0.2121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3313\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3292\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3201\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2776\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2479\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1989\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.251\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2183\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1972\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:  39%|███▉      | 1020/2586 [01:08<01:46, 14.69it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 14.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Elapsed time 208.341\n",
      "    total training loss 2.0006\n",
      "    extract 2.1581\n",
      "    joint_embedding 0.2547\n",
      "    compose2 0.7237\n",
      "    compose1 0.204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3404\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3547\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3961\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.3047\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2483\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.208\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2681\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2296\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2045\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 17.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Elapsed time 207.5353\n",
      "    total training loss 2.2235\n",
      "    extract 2.175\n",
      "    joint_embedding 0.2602\n",
      "    compose2 0.7563\n",
      "    compose1 0.2032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.343\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3343\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3188\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.281\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2459\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.1996\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2583\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2183\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2011\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Elapsed time 208.1862\n",
      "    total training loss 2.0368\n",
      "    extract 2.1871\n",
      "    joint_embedding 0.2437\n",
      "    compose2 0.7671\n",
      "    compose1 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3437\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3933\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3558\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2901\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2473\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.2021\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2519\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2254\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2039\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   6%|▋         | 166/2586 [00:12<02:36, 15.46it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 14.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Elapsed time 208.5339\n",
      "    total training loss 2.0579\n",
      "    extract 2.155\n",
      "    joint_embedding 0.2651\n",
      "    compose2 0.7322\n",
      "    compose1 0.2135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3536\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3201\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3441\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.3206\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2571\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.2156\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2624\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2219\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2037\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Elapsed time 207.8271\n",
      "    total training loss 1.9647\n",
      "    extract 2.1623\n",
      "    joint_embedding 0.2319\n",
      "    compose2 0.7491\n",
      "    compose1 0.1847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.357\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.315\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3056\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2761\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2568\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.218\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2761\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2193\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2118\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Elapsed time 207.6584\n",
      "    total training loss 2.1855\n",
      "    extract 2.1583\n",
      "    joint_embedding 0.2502\n",
      "    compose2 0.7202\n",
      "    compose1 0.2115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3589\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3712\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3068\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2796\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2444\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.2033\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2708\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2104\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.1988\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:  17%|█▋        | 432/2586 [00:30<02:32, 14.13it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Elapsed time 208.9472\n",
      "    total training loss 2.0658\n",
      "    extract 2.1197\n",
      "    joint_embedding 0.2355\n",
      "    compose2 0.7165\n",
      "    compose1 0.1923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3602\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3638\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.315\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2668\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.249\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.2112\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2775\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2108\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2076\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Elapsed time 208.0926\n",
      "    total training loss 2.0059\n",
      "    extract 2.1406\n",
      "    joint_embedding 0.2461\n",
      "    compose2 0.7268\n",
      "    compose1 0.2199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3644\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3944\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3611\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2976\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.263\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.2201\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2665\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2172\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2018\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:  78%|███████▊  | 2012/2586 [02:16<00:38, 15.10it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "training 1 epoch: 100%|██████████| 2586/2586 [02:55<00:00, 14.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Elapsed time 208.4446\n",
      "    total training loss 2.0243\n",
      "    extract 2.1262\n",
      "    joint_embedding 0.2299\n",
      "    compose2 0.7044\n",
      "    compose1 0.2036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3682\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3734\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3301\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.268\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2591\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.2163\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2831\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2185\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2133\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Elapsed time 207.986\n",
      "    total training loss 1.9928\n",
      "    extract 2.1305\n",
      "    joint_embedding 0.234\n",
      "    compose2 0.7034\n",
      "    compose1 0.1852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3703\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3564\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3451\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.275\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2533\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.2138\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2793\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2179\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2036\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:  75%|███████▍  | 1927/2586 [02:11<00:40, 16.19it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 14.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Elapsed time 207.4148\n",
      "    total training loss 2.0347\n",
      "    extract 2.1444\n",
      "    joint_embedding 0.2369\n",
      "    compose2 0.7141\n",
      "    compose1 0.1946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3741\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3451\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.4033\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2939\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2549\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.2126\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2873\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2201\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.207\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 2586/2586 [02:54<00:00, 15.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Elapsed time 207.9135\n",
      "    total training loss 2.026\n",
      "    extract 2.1293\n",
      "    joint_embedding 0.2316\n",
      "    compose2 0.7109\n",
      "    compose1 0.1788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:   0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3723\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3434\n",
      "  SimpleImageCaptions112 svl_ttt_recall_top1 0.3013\n",
      "  SimpleImageCaptions112 svl_tti_recall_top1 0.2793\n",
      "  SimpleImageCaptions112 svl_tit_recall_top1 0.2481\n",
      "  SimpleImageCaptions112 svl_tii_recall_top1 0.2152\n",
      "  SimpleImageCaptions112 svl_itt_recall_top1 0.2847\n",
      "  SimpleImageCaptions112 svl_iti_recall_top1 0.2199\n",
      "  SimpleImageCaptions112 svl_iit_recall_top1 0.2114\n",
      "  SimpleImageCaptions112 svl_iii_recall_top1 0.1614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch:  56%|█████▌    | 1453/2586 [01:38<01:13, 15.49it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "losses_tracking = {}\n",
    "epoch = 0\n",
    "tic = time.time()\n",
    "while True:\n",
    "\n",
    "    # show stat, training losses\n",
    "    print 'Epoch', epoch, 'Elapsed time', round(time.time() - tic, 4)\n",
    "    tic = time.time()\n",
    "    for loss_name in losses_tracking:\n",
    "        avg_loss = np.mean(losses_tracking[loss_name][-250:])\n",
    "        print '   ', loss_name, round(avg_loss, 4)\n",
    "        logger.add_scalar(loss_name, avg_loss, epoch)\n",
    "\n",
    "    # test\n",
    "    tests = []\n",
    "    for dataset in [trainset, sic112]:\n",
    "        t = test(model, dataset, opt)\n",
    "        tests += [(dataset.name() + ' ' + metric_name, metric_value) for metric_name, metric_value in t]\n",
    "    for metric_name, metric_value in tests:\n",
    "        print ' ', metric_name, round(metric_value, 4)\n",
    "        logger.add_scalar(metric_name, metric_value, epoch)\n",
    "\n",
    "    # train\n",
    "    if epoch >= opt.num_epochs:\n",
    "        break\n",
    "    train_1_epoch(model, optimizer, trainset, opt, losses_tracking,\n",
    "                  add_extract_compose_losses = epoch>=1)\n",
    "    epoch += 1\n",
    "\n",
    "    # learing rate scheduling\n",
    "    if epoch % opt.learning_rate_decay_frequency == 0:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
