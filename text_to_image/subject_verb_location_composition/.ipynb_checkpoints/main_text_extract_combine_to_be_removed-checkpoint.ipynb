{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import json\n",
    "import random\n",
    "import PIL\n",
    "from tqdm import tqdm as tqdm\n",
    "import sys\n",
    "\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    %matplotlib inline  \n",
    "    plt.rcParams['figure.figsize'] = (10, 10)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append('./../cvpr19_refactored_codes')\n",
    "import text_model\n",
    "import torch_functions\n",
    "\n",
    "torch.set_num_threads(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_opt():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('-f', type=str, default='f')\n",
    "    parser.add_argument('-comment', type=str, default='_test_main_text_extract_combine')\n",
    "    parser.add_argument('--embed_dim', type=int, default=512)\n",
    "    parser.add_argument('--loader_num_workers', type=int, default=8)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--optim', type=str, default='adam', help='what update to use? sgd|adam')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-3)\n",
    "    parser.add_argument('--learning_rate_decay_frequency', type=int, default=50000)\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-05)\n",
    "    parser.add_argument('--momentum', type=float, default=0.9)\n",
    "    parser.add_argument('--num_epochs', type=int, default=50)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    return args\n",
    "\n",
    "opt = parse_opt() \n",
    "logger = SummaryWriter(comment = opt.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NamsBaseDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def name(self):\n",
    "        assert(False)\n",
    "        \n",
    "    def get_image_path(self, idx):\n",
    "        assert(False)\n",
    "    \n",
    "    def get_image_captions(self, idx):\n",
    "        assert(False)\n",
    "        \n",
    "    def get_loader(self, batch_size, shuffle = False, drop_last = False, num_workers = 0):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = shuffle,\n",
    "            num_workers = num_workers,\n",
    "            drop_last = drop_last,\n",
    "            collate_fn = lambda i: i\n",
    "        )\n",
    "    \n",
    "    def normalize_caption(self, text):\n",
    "        import string\n",
    "        if type(text) == str:\n",
    "            text = unicode(text, \"utf-8\")\n",
    "        text = text.encode('ascii', 'replace')\n",
    "        text = str(text).lower().translate(None, string.punctuation).strip()\n",
    "        return text\n",
    "    \n",
    "    def get_all_texts(self):\n",
    "        texts = []\n",
    "        for i in range(len(self)):\n",
    "            for t in self.get_image_captions(i):\n",
    "                texts.append(t)\n",
    "        return texts\n",
    "    \n",
    "    def precompute_img_features(self, force=False):\n",
    "        features_filename = self.name() + '_features.npy'\n",
    "        try:\n",
    "            assert(not force)\n",
    "            self.img_features = np.load(features_filename)\n",
    "            print 'sucessfully loaded features'\n",
    "            return\n",
    "        except:\n",
    "            print 'compute features...'\n",
    "        self.img_features = None\n",
    "        \n",
    "        # run model on all images\n",
    "        net = torchvision.models.resnet50(pretrained=True)\n",
    "        net.avgpool = torch.nn.AdaptiveAvgPool2d((1,1))\n",
    "        net.fc = torch.nn.Dropout()\n",
    "        net = net.cuda().eval()\n",
    "        loader = self.get_loader(batch_size=8, shuffle=False, drop_last=False, num_workers=4)\n",
    "        img_features = np.zeros((len(self), 2048))\n",
    "        i = 0\n",
    "        for data in tqdm(loader):\n",
    "            imgs = torch.stack([d['image'] for d in data])\n",
    "            x = net(imgs.cuda()).cpu().detach().numpy()\n",
    "            img_features[i:(i+x.shape[0]),:] = x\n",
    "            imgs = torch.flip(imgs, [3])\n",
    "            x = net(imgs.cuda()).cpu().detach().numpy()\n",
    "            img_features[i:(i+x.shape[0]),:] += x\n",
    "            i += x.shape[0]\n",
    "        self.img_features = img_features\n",
    "        np.save(features_filename, self.img_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.img_features is not None:\n",
    "            img = self.img_features[idx,:]\n",
    "        else:\n",
    "            raw_img = torchvision.datasets.folder.pil_loader(self.get_image_path(idx))\n",
    "            img = raw_img\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        img = {\n",
    "            'id': None,\n",
    "            'label': None,\n",
    "            'index': idx,\n",
    "            'image': img,\n",
    "            'captions': self.get_image_captions(idx)\n",
    "        }\n",
    "        return img\n",
    "\n",
    "# Required files:\n",
    "# dataset_path/annotations/captions_val2014.json\n",
    "# dataset_path/annotations/captions_train2014.json\n",
    "# dataset_path/train2014/[image files]\n",
    "# dataset_path/val2014/[image files]\n",
    "class COCOCaptionDataset(NamsBaseDataset):\n",
    "    \n",
    "    def __init__(self, dataset_path = '', transform = None, test_split = False):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform\n",
    "        self.test_split = test_split\n",
    "        \n",
    "        import json\n",
    "        if test_split:\n",
    "            x = json.load(open(self.dataset_path + '/annotations/captions_val2014.json', 'rt'))\n",
    "            img_path = dataset_path + '/val2014/'\n",
    "        else:\n",
    "            x = json.load(open(self.dataset_path + '/annotations/captions_train2014.json', 'rt'))\n",
    "            img_path = dataset_path + '/train2014/'\n",
    "            \n",
    "        imgs = []\n",
    "        id2id = {}\n",
    "        for img in x['images']:\n",
    "            id2id[img['id']] = len(imgs)\n",
    "            imgs += [{\n",
    "                'id': img['id'],\n",
    "                'class': img['id'],\n",
    "                'filename': img_path + img['file_name'],\n",
    "                'captions': []\n",
    "            }]\n",
    "\n",
    "        for cap in x['annotations']:\n",
    "            imgs[id2id[cap['image_id']]]['captions'] += [self.normalize_caption(cap['caption'])]\n",
    "\n",
    "        self.imgs = imgs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def name(self):\n",
    "        if self.test_split:\n",
    "            return \"CocoCapTest\"\n",
    "        return \"CocoCapTrain\"\n",
    "        \n",
    "    def get_image_path(self, idx):\n",
    "        return self.imgs[idx]['filename']\n",
    "    \n",
    "    def get_image_captions(self, idx):\n",
    "        return self.imgs[idx]['captions']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = super(COCOCaptionDataset, self).__getitem__(idx)\n",
    "        item['label'] = self.imgs[idx]['id']\n",
    "        return item\n",
    "        \n",
    "class SimpleImageCaptions112(NamsBaseDataset):\n",
    "    \n",
    "    def __init__(self, dataset_path = '', transform = None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform\n",
    "        imgs = []\n",
    "        import os\n",
    "        import os.path\n",
    "        for d in os.listdir(dataset_path):\n",
    "          if not os.path.isfile(dataset_path + '/' + d):\n",
    "            for f in os.listdir(dataset_path + '/' + d):\n",
    "              if os.path.isfile(dataset_path + '/' + d + '/' + f):\n",
    "                imgs += [{\n",
    "                    'id': len(imgs),\n",
    "                    'captions': [self.normalize_caption(d)],\n",
    "                    'filename': dataset_path + '/' + d + '/' + f\n",
    "                }]\n",
    "        self.imgs = imgs\n",
    "        self.make_test_queries()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"SimpleImageCaptions112\"\n",
    "        \n",
    "    def get_image_path(self, idx):\n",
    "        return self.imgs[idx]['filename']\n",
    "    \n",
    "    def get_image_captions(self, idx):\n",
    "        return self.imgs[idx]['captions']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = super(SimpleImageCaptions112, self).__getitem__(idx)\n",
    "        item['label'] = self.imgs[idx]['captions'][0]\n",
    "        return item\n",
    "    \n",
    "    def make_test_queries(self):\n",
    "        \n",
    "        novel_obj_list = ['trex', 'stormtrooper', 'darthvader', 'chewbacca']\n",
    "        \n",
    "        caption2ids = {}\n",
    "        for i in range(len(self)):\n",
    "            for caption in self.get_image_captions(i):\n",
    "                try:\n",
    "                    caption2ids[caption] += [i]\n",
    "                except:\n",
    "                    caption2ids[caption] = []\n",
    "                    caption2ids[caption] += [i]\n",
    "        \n",
    "        test_queries = []\n",
    "        for cap1 in caption2ids.keys():\n",
    "            for cap2 in caption2ids.keys():\n",
    "                cap1s = cap1.replace('on the ', '').replace('in the ', '').replace('living room', 'livingroom').split()\n",
    "                cap2s = cap2.replace('on the ', '').replace('in the ', '').replace('living room', 'livingroom').split()\n",
    "                diffs = []\n",
    "                for w1, w2 in zip(cap1s, cap2s):\n",
    "                    if w1 != w2:\n",
    "                        w1 = w1.replace('livingroom', 'living room')\n",
    "                        w2 = w2.replace('livingroom', 'living room')\n",
    "                        diffs += [w1, w2]\n",
    "                if len(diffs) != 2:\n",
    "                    continue\n",
    "                if diffs[0] in novel_obj_list or diffs[1] in novel_obj_list:\n",
    "                    continue\n",
    "                for idx in caption2ids[cap1]:\n",
    "                    test_queries += [{\n",
    "                        'source_idx': idx,\n",
    "                        'source_caption': cap1,\n",
    "                        'target_caption': cap2,\n",
    "                        'replacing_words': diffs\n",
    "                    }]\n",
    "        self.test_queries_seen_objects = []\n",
    "        self.test_queries_novel_objects = []\n",
    "        for t in test_queries:\n",
    "            novel_objects = False\n",
    "            for w in novel_obj_list:\n",
    "                if w in t['source_caption']:\n",
    "                    novel_objects = True\n",
    "            if novel_objects:\n",
    "                self.test_queries_novel_objects += [t]\n",
    "            else:\n",
    "                self.test_queries_seen_objects += [t]\n",
    "        assert(len(self.test_queries_seen_objects) == 18051)\n",
    "        assert(len(self.test_queries_novel_objects) == 745)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    def make_test_queriesxxxx(self):\n",
    "        \n",
    "        novel_obj_list = ['trex', 'stormtrooper', 'darthvader', 'chewbacca']\n",
    "        \n",
    "        random.seed(333)\n",
    "        test_queries = []\n",
    "        while len(test_queries) < 500000:\n",
    "            \n",
    "            img1 = self.imgs[len(test_queries) % len(self.imgs)]\n",
    "            cap1 = img1['captions'][0]\n",
    "            img2 = random.choice(self.imgs)\n",
    "            cap2 = img2['captions'][0]\n",
    "            \n",
    "            cap1 = cap1.replace('on the ', '').replace('in the ', '').replace('living room', 'livingroom')\n",
    "            cap1 = cap1.split()\n",
    "            cap2 = cap2.replace('on the ', '').replace('in the ', '').replace('living room', 'livingroom')\n",
    "            cap2 = cap2.split()\n",
    "\n",
    "            diffs = []\n",
    "            for w1, w2 in zip(cap1, cap2):\n",
    "                if w1 != w2:\n",
    "                    w1 = w1.replace('livingroom', 'living room')\n",
    "                    w2 = w2.replace('livingroom', 'living room')\n",
    "                    diffs += [w1, w2]\n",
    "            if len(diffs) != 2:\n",
    "                continue\n",
    "            if diffs[0] in novel_obj_list or diffs[1] in novel_obj_list:\n",
    "                continue\n",
    "\n",
    "            ix = img1['id']\n",
    "            assert(self.imgs[ix] == img1)\n",
    "            test_queries += [{\n",
    "                'ix': ix,\n",
    "                'source_idx': img1['id'],\n",
    "                'replacing_words': diffs,\n",
    "                'source_caption': img1['captions'][0],\n",
    "                'target_caption': img2['captions'][0]\n",
    "            }]\n",
    "\n",
    "        # save\n",
    "        query_check = {}\n",
    "        self.test_queries = []\n",
    "        self.novel_obj_test_queries = []\n",
    "        for q in test_queries:\n",
    "            k = str(q['source_idx']) + q['target_caption']\n",
    "            if k in query_check:\n",
    "                continue\n",
    "            query_check[k] = True\n",
    "            contain_novel_obj = False\n",
    "            for w in novel_obj_list:\n",
    "                if w in q['source_caption'] or w in q['target_caption']:\n",
    "                    contain_novel_obj = True\n",
    "            if contain_novel_obj:\n",
    "                self.novel_obj_test_queries += [q]\n",
    "            else:\n",
    "                self.test_queries += [q]\n",
    "        self.test_queries_novel_objects = self.novel_obj_test_queries[:700]\n",
    "        self.test_queries_seen_objects = self.test_queries[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = '/home/nam/exp/Fall2018/imagetextdatasets/coco'\n",
    "trainset = COCOCaptionDataset(\n",
    "    dataset_path,\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(336, scale=(0.8, 1.0), ratio=(0.75, 1.3)),\n",
    "        #torchvision.transforms.Resize((336,336)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    test_split = False\n",
    ")\n",
    "\n",
    "testset = COCOCaptionDataset(\n",
    "    dataset_path,\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(336, scale=(0.8, 1.0), ratio=(0.75, 1.3)),\n",
    "        #torchvision.transforms.Resize((336,336)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    test_split = True\n",
    ")\n",
    "\n",
    "sic112 = SimpleImageCaptions112(\n",
    "    '../imagetextdatasets/nams_googleimage_dataset/googleimagesdata/downloads12',\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(336, scale=(0.8, 1.0), ratio=(0.75, 1.3)),\n",
    "        #torchvision.transforms.Resize((336,336)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read subjects verbs contexts from cocotrain2014.nams.pth and add to trainset\n"
     ]
    }
   ],
   "source": [
    "print 'Read subjects verbs contexts from cocotrain2014.nams.pth and add to trainset'\n",
    "\n",
    "x = torch.load('cocotrain2014.nams.pth')\n",
    "\n",
    "id2id = {}\n",
    "for i, img in enumerate(trainset.imgs):\n",
    "    id2id[img['id']] = i\n",
    "for img in x['images']:\n",
    "    \n",
    "    sim = img['subjects_sim']\n",
    "    if sim is None:\n",
    "        sim = np.eye(1)\n",
    "    a = np.sum((sim - np.eye(sim.shape[0])) > 0.7, axis=0)\n",
    "    a = np.where(a > 0)[0]\n",
    "    subjects = [img['subjects'][i] for i in a]\n",
    "    \n",
    "    sim = img['verbs_sim']\n",
    "    if sim is None:\n",
    "        sim = np.eye(1)\n",
    "    a = np.sum((sim - np.eye(sim.shape[0])) > 0.7, axis=0)\n",
    "    a = np.where(a > 0)[0]\n",
    "    verbs = [img['verbs'][i] for i in a]\n",
    "    \n",
    "    sim = img['contexts_sim']\n",
    "    if sim is None:\n",
    "        sim = np.eye(1)\n",
    "    a = np.sum((sim - np.eye(sim.shape[0])) > 0.7, axis=0)\n",
    "    a = np.where(a > 0)[0]\n",
    "    contexts = [img['contexts'][i] for i in a]\n",
    "    \n",
    "    trainset.imgs[id2id[img['id']]]['subjects'] = subjects\n",
    "    trainset.imgs[id2id[img['id']]]['verbs'] = verbs\n",
    "    trainset.imgs[id2id[img['id']]]['contexts'] = contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in sic112.imgs:\n",
    "    words = img['captions'][0].split()\n",
    "    img['subjects'] = [words[0]]\n",
    "    if not words[0] in ['trex', 'stormtrooper', 'darthvader', 'chewbacca']:\n",
    "        img['verbs'] = [words[1]]\n",
    "        img['contexts'] = [' '.join(words[2:])]\n",
    "    else:\n",
    "        img['verbs'] = []\n",
    "        img['contexts'] = [' '.join(words[1:])]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucessfully loaded features\n",
      "sucessfully loaded features\n",
      "sucessfully loaded features\n"
     ]
    }
   ],
   "source": [
    "trainset.precompute_img_features(0)\n",
    "testset.precompute_img_features(0)\n",
    "sic112.precompute_img_features(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = torch.nn.Module()\n",
    "\n",
    "# image\n",
    "model.img_encoder = torchvision.models.resnet50(pretrained=True)\n",
    "model.img_encoder.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.Linear(2048, 2048),\n",
    "    torch.nn.BatchNorm1d(2048),\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(2048, opt.embed_dim)\n",
    ")\n",
    "\n",
    "# text\n",
    "model.text_encoder = text_model.TextLSTMModel(\n",
    "    texts_to_build_vocab = trainset.get_all_texts(),\n",
    "    word_embed_dim = 256,\n",
    "    lstm_hidden_dim = 512\n",
    ")\n",
    "model.text_encoder.fc_output = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.1),\n",
    "    torch.nn.Linear(opt.embed_dim, 2048),\n",
    "    torch.nn.BatchNorm1d(2048),\n",
    "    torch.nn.Dropout(0.1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(2048, opt.embed_dim)\n",
    ")\n",
    "\n",
    "# transform function\n",
    "class One2OneTransformation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(One2OneTransformation, self).__init__()\n",
    "        embed_dim = opt.embed_dim\n",
    "        self.m = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_dim * 1, embed_dim * 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 2, embed_dim * 2),\n",
    "            torch.nn.BatchNorm1d(embed_dim * 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 2, embed_dim)\n",
    "        )\n",
    "        self.norm = torch_functions.NormalizationLayer(learn_scale=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.norm(x)\n",
    "        f = self.m(f)\n",
    "        return f\n",
    "    \n",
    "class Three2OneTransformation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Three2OneTransformation, self).__init__()\n",
    "        embed_dim = opt.embed_dim\n",
    "        self.m = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_dim * 3, embed_dim * 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 5, embed_dim * 5),\n",
    "            torch.nn.BatchNorm1d(embed_dim * 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embed_dim * 5, embed_dim)\n",
    "        )\n",
    "        self.norm = torch_functions.NormalizationLayer(learn_scale=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = torch.cat([self.norm(i) for i in x], dim=1)\n",
    "        f = self.m(f)\n",
    "        return f\n",
    "\n",
    "model.subject_extractor = One2OneTransformation()\n",
    "model.verb_extractor = One2OneTransformation()\n",
    "model.context_extractor = One2OneTransformation()\n",
    "model.svc_combine = Three2OneTransformation()\n",
    "\n",
    "model.snorm = torch_functions.NormalizationLayer(normalize_scale=4.0, learn_scale=True)\n",
    "model = model.cuda()\n",
    "\n",
    "# loss function\n",
    "def pair_loss(a, b):\n",
    "    # force a,b similar in the embedding space\n",
    "    a = model.snorm(a)\n",
    "    b = model.snorm(b).transpose(0,1)\n",
    "    x = torch.mm(a, b)\n",
    "    if random.random() > 0.5:\n",
    "        x = x.transpose(0, 1)\n",
    "    labels = torch.tensor(range(x.shape[0])).long()\n",
    "    return torch.nn.functional.cross_entropy(x, labels.cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "params = []\n",
    "params.append({'params': [p for p in model.img_encoder.fc.parameters()]})\n",
    "params.append({'params': [p for p in model.img_encoder.parameters()], 'lr': 0.1 * opt.learning_rate})\n",
    "params.append({'params': [p for p in model.text_encoder.parameters()], 'lr': opt.learning_rate})\n",
    "params.append({'params': [p for p in model.parameters()]})\n",
    "\n",
    "# remove dup params (keep the first one)\n",
    "for i1, p1 in enumerate(params):\n",
    "  for i2, p2 in enumerate(params):\n",
    "    if p1 is not p2:\n",
    "      for p11 in p1['params']:\n",
    "        for j, p22 in enumerate(p2['params']):\n",
    "          if p11 is p22:\n",
    "            p2['params'][j] = torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=opt.learning_rate,\n",
    "    momentum=opt.momentum,\n",
    "    weight_decay=opt.weight_decay\n",
    ")\n",
    "if opt.optim == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params,\n",
    "        lr=opt.learning_rate,\n",
    "        weight_decay=opt.weight_decay\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(testset):\n",
    "    r = test_text_to_image_retrieval(testset)\n",
    "    if testset == sic112:\n",
    "        r += test_svc(testset)\n",
    "    return r\n",
    "\n",
    "def test_text_to_image_retrieval(testset):\n",
    "    model.eval()\n",
    "    img_features = []\n",
    "    img_labels = []\n",
    "    text_features = []\n",
    "    text_labels = []\n",
    "    for data in testset.get_loader(batch_size = opt.batch_size, shuffle = True, drop_last= False):\n",
    "        # extract image features\n",
    "        imgs = np.stack([d['image'] for d in data])\n",
    "        imgs = torch.from_numpy(imgs).float()\n",
    "        if len(imgs.shape) == 2:\n",
    "            imgs = model.img_encoder.fc(imgs.cuda())\n",
    "        else:\n",
    "            imgs = model.img_encoder(imgs.cuda())\n",
    "        imgs = model.snorm(imgs).cpu().detach().numpy()\n",
    "        img_features += [imgs]\n",
    "        img_labels += [d['label'] for d in data]\n",
    "\n",
    "        # text\n",
    "        texts = []\n",
    "        for d in data:\n",
    "            texts += d['captions']\n",
    "            text_labels += [d['label'] for c in d['captions']]\n",
    "        texts = model.text_encoder(texts)\n",
    "        texts = model.snorm(texts).cpu().detach().numpy()\n",
    "        text_features += [texts]\n",
    "\n",
    "        if len(img_labels) > 1100:\n",
    "            break\n",
    "\n",
    "    img_features = np.concatenate(img_features, axis=0)\n",
    "    text_features = np.concatenate(text_features, axis=0)\n",
    "\n",
    "    \n",
    "    # text to image\n",
    "    sims = text_features.dot(img_features.T)\n",
    "    r1 = 0.0\n",
    "    for i in range(sims.shape[0]):\n",
    "        s = -sims[i,:]\n",
    "        s = np.argsort(s)\n",
    "        if text_labels[i] == img_labels[s[0]]:\n",
    "            r1 += 1\n",
    "    r1 /= sims.shape[0]\n",
    "    return [('text2image_recall_top1', r1)]\n",
    "\n",
    "\n",
    "def test_svc(sic112):\n",
    "    model.eval()\n",
    "\n",
    "    # all img features\n",
    "    img_features = []\n",
    "    for data in sic112.get_loader(batch_size = opt.batch_size, shuffle = False, drop_last= False):\n",
    "        # extract image features\n",
    "        imgs = np.stack([d['image'] for d in data])\n",
    "        imgs = torch.from_numpy(imgs).float()\n",
    "        if len(imgs.shape) == 2:\n",
    "            imgs = model.img_encoder.fc(imgs.cuda())\n",
    "        else:\n",
    "            imgs = model.img_encoder(imgs.cuda())\n",
    "        imgs = model.snorm(imgs).cpu().detach().numpy()\n",
    "        img_features += [imgs]\n",
    "\n",
    "    img_features = np.concatenate(img_features, axis=0)\n",
    "    img_labels = [img['captions'][0] for img in sic112.imgs]\n",
    "    \n",
    "    # construct random queries\n",
    "    queries = []\n",
    "    np.random.seed(123)\n",
    "    while len(queries) < 10000:\n",
    "        i = np.random.randint(0, len(sic112.imgs))\n",
    "        j = np.random.randint(0, len(sic112.imgs))\n",
    "        k = np.random.randint(0, len(sic112.imgs))\n",
    "        if len(sic112.imgs[i]['verbs'] + sic112.imgs[j]['verbs'] + sic112.imgs[k]['verbs']) < 3:\n",
    "            continue\n",
    "        queries += [{\n",
    "            'subject_img_id': i,\n",
    "            'verb_img_id': j,\n",
    "            'context_img_id': k,\n",
    "            'subject': sic112.imgs[i]['subjects'][0],\n",
    "            'verb': sic112.imgs[j]['verbs'][0],\n",
    "            'context': sic112.imgs[k]['contexts'][0],\n",
    "            'label': sic112.imgs[i]['subjects'][0] + ' ' + sic112.imgs[j]['verbs'][0] + ' ' + sic112.imgs[k]['contexts'][0]\n",
    "        }]\n",
    "        \n",
    "    #----\n",
    "    #----\n",
    "    r = []\n",
    "    for s in ['t', 'i']:\n",
    "      for v in ['t', 'i']:\n",
    "        for c in ['t', 'i']:\n",
    "            # compute query features\n",
    "            query_features = []\n",
    "            query_labels = []\n",
    "            for i in range(0, len(queries), opt.batch_size):\n",
    "                if s == 'i':\n",
    "                    subjects = model.subject_extractor(torch.from_numpy(\n",
    "                        img_features[[q['subject_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                    ).cuda())\n",
    "                else:\n",
    "                    subjects = model.text_encoder([q['subject'] for q in queries[i:(i+opt.batch_size)]])\n",
    "                if v == 'i':\n",
    "                    verbs = model.subject_extractor(torch.from_numpy(\n",
    "                        img_features[[q['verb_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                    ).cuda())\n",
    "                else:\n",
    "                    verbs = model.text_encoder([q['verb'] for q in queries[i:(i+opt.batch_size)]])\n",
    "                if c == 'i':\n",
    "                    contexts = model.subject_extractor(torch.from_numpy(\n",
    "                        img_features[[q['context_img_id'] for q in queries[i:(i+opt.batch_size)]],:]\n",
    "                    ).cuda())\n",
    "                else:\n",
    "                    contexts = model.text_encoder([q['context'] for q in queries[i:(i+opt.batch_size)]])\n",
    "                svc = model.svc_combine([subjects, verbs, contexts])\n",
    "                svc = svc.cpu().detach().numpy()\n",
    "                query_features += [svc]\n",
    "                query_labels += [q['label'] for q in queries[i:(i+opt.batch_size)]]\n",
    "\n",
    "            query_features = np.concatenate(query_features, axis=0)\n",
    "\n",
    "            # compute recall\n",
    "            def measure_retrieval_performance(query_features, name = 'X'):\n",
    "                sims = query_features.dot(img_features.T)\n",
    "                sims = sims\n",
    "                for k in [1, 5, 10]:\n",
    "                    r1 = 0.0\n",
    "                    for i in range(sims.shape[0]):\n",
    "                        s = -sims[i,:]\n",
    "                        s = np.argsort(s)\n",
    "                        if query_labels[i] in [img_labels[s[j]] for j in range(k)]:\n",
    "                        #if query_labels[i] == img_labels[s[0]]:\n",
    "                            r1 += 1\n",
    "                    r1 /= sims.shape[0]\n",
    "                    r.append(('svc_' + name + '_recall_top' + str(k), r1))\n",
    "                return r\n",
    "            measure_retrieval_performance(query_features, name = s + v + c)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses_tracking = {}\n",
    "it = 0\n",
    "epoch = 0\n",
    "tic = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 0 epoch 0 Elapsed time 0.3425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.0007\n",
      "  CocoCapTest text2image_recall_top1 0.0005\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.0116\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.0237\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.0489\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.0699\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.012\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.0489\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.1045\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.012\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.0699\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.0699\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.012\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.0489\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.0844\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.0117\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.0489\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.0699\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.0117\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.0494\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.0845\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.012\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.0489\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.0639\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.012\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.0489\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:42<00:00, 60.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 2586 epoch 1 Elapsed time 76.154\n",
      "    total training loss 0.8041\n",
      "    joint_embedding 0.8041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1405\n",
      "  CocoCapTest text2image_recall_top1 0.1242\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2018\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.0054\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.024\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.0573\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.0054\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.024\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.0562\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.0054\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.024\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.0283\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0054\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.024\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.0353\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.0054\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.024\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.0446\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.0054\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.024\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.0353\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.0054\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.024\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.0353\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0054\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.024\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:09<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 5172 epoch 2 Elapsed time 281.8792\n",
      "    svc combine loss 2 0.2849\n",
      "    total training loss 2.4246\n",
      "    extractor loss 1 1.5718\n",
      "    joint_embedding 0.5679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.1824\n",
      "  CocoCapTest text2image_recall_top1 0.1814\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2304\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.2437\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.6488\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8331\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.0719\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.2453\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.3645\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1415\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4243\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.5618\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0469\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.194\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.3131\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.1915\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.5286\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.672\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.0505\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.1939\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.2883\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1125\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.3291\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.4595\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0418\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.1465\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.2377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:07<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 7758 epoch 3 Elapsed time 280.1595\n",
      "    svc combine loss 2 0.242\n",
      "    total training loss 2.1435\n",
      "    extractor loss 1 1.4162\n",
      "    joint_embedding 0.4853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2152\n",
      "  CocoCapTest text2image_recall_top1 0.1943\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.233\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3178\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.6074\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.7941\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1306\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.3772\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.5102\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1414\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4295\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.576\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0545\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2131\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.3285\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2102\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.5322\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.6958\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.0999\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3008\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4201\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1229\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.3777\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5201\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0516\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.185\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.2814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:08<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 10344 epoch 4 Elapsed time 281.2953\n",
      "    svc combine loss 2 0.2151\n",
      "    total training loss 1.9834\n",
      "    extractor loss 1 1.3386\n",
      "    joint_embedding 0.4297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2423\n",
      "  CocoCapTest text2image_recall_top1 0.2058\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3009\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3158\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.6283\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.7778\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1214\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.3497\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.4625\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1489\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4064\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.55\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0754\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2454\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.3631\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2031\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.5051\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.6659\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.0844\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.2572\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.3613\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1103\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.3493\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.4855\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0623\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.1935\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.2911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:04<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 12930 epoch 5 Elapsed time 277.3875\n",
      "    svc combine loss 2 0.2054\n",
      "    total training loss 1.9108\n",
      "    extractor loss 1 1.303\n",
      "    joint_embedding 0.4024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.252\n",
      "  CocoCapTest text2image_recall_top1 0.2338\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2875\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3553\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.6893\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8613\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1456\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.4143\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.5767\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1622\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4178\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.5539\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.086\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2647\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4047\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2201\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.577\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7484\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1193\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3559\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4947\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1219\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.3686\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5168\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0678\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2293\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:05<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 15516 epoch 6 Elapsed time 279.3127\n",
      "    svc combine loss 2 0.1844\n",
      "    total training loss 1.8293\n",
      "    extractor loss 1 1.263\n",
      "    joint_embedding 0.3819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2635\n",
      "  CocoCapTest text2image_recall_top1 0.2347\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.267\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.2072\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.6356\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8216\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1404\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.3987\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.5496\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1594\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.401\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.534\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0863\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.287\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.412\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.1922\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.5332\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.689\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1069\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3112\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4431\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1241\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.3583\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.4932\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0744\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2273\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:04<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 18102 epoch 7 Elapsed time 277.2155\n",
      "    svc combine loss 2 0.1763\n",
      "    total training loss 1.7854\n",
      "    extractor loss 1 1.2357\n",
      "    joint_embedding 0.3734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2839\n",
      "  CocoCapTest text2image_recall_top1 0.2416\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3116\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.2683\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7245\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8419\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1546\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.3951\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.5315\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.139\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4006\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.5484\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0697\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2352\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.3658\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2366\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6305\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7914\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1256\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3447\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4809\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1317\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.3871\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5209\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0611\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2184\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:08<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 20688 epoch 8 Elapsed time 281.4295\n",
      "    svc combine loss 2 0.1749\n",
      "    total training loss 1.753\n",
      "    extractor loss 1 1.2156\n",
      "    joint_embedding 0.3625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.2808\n",
      "  CocoCapTest text2image_recall_top1 0.2549\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2518\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.2599\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.602\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.766\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1262\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.3878\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.5449\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1417\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.401\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.5367\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0727\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2461\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.3735\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2289\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6046\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7731\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1152\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3517\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4947\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1422\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4067\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5479\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0652\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2153\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:03<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 23274 epoch 9 Elapsed time 276.5551\n",
      "    svc combine loss 2 0.1667\n",
      "    total training loss 1.71\n",
      "    extractor loss 1 1.1935\n",
      "    joint_embedding 0.3498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3128\n",
      "  CocoCapTest text2image_recall_top1 0.2803\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3464\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.357\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.6849\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8267\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.133\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.4072\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.5581\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1621\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4392\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.5837\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.096\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3001\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4263\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2396\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.5967\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7661\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1103\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3447\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4827\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1341\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.3891\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5395\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0709\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2338\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:07<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 25860 epoch 10 Elapsed time 280.4806\n",
      "    svc combine loss 2 0.158\n",
      "    total training loss 1.6774\n",
      "    extractor loss 1 1.1779\n",
      "    joint_embedding 0.3415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3152\n",
      "  CocoCapTest text2image_recall_top1 0.2691\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2625\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3374\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7151\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8851\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1251\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.413\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.5629\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1674\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.51\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.665\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0779\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2774\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4201\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2289\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6385\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8042\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1002\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3307\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4757\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1373\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4507\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.6179\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0614\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2196\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:07<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 28446 epoch 11 Elapsed time 280.0226\n",
      "    svc combine loss 2 0.1559\n",
      "    total training loss 1.6636\n",
      "    extractor loss 1 1.1756\n",
      "    joint_embedding 0.3321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3061\n",
      "  CocoCapTest text2image_recall_top1 0.2696\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3027\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.2412\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.6227\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8285\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1425\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.4281\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.5843\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1501\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.3736\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.5073\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0782\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2547\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.3773\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2231\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.553\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7209\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.108\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3357\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4788\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1373\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.3661\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5001\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0644\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2285\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:06<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 31032 epoch 12 Elapsed time 279.9527\n",
      "    svc combine loss 2 0.1484\n",
      "    total training loss 1.6206\n",
      "    extractor loss 1 1.1538\n",
      "    joint_embedding 0.3183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.304\n",
      "  CocoCapTest text2image_recall_top1 0.2768\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3295\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.2053\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.5958\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.7877\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1386\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.4025\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.5662\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1325\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.3515\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.4857\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0836\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2655\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.3831\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2112\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.5304\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.6884\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1106\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.351\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4949\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1147\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.3391\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.4808\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0614\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2123\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:08<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 33618 epoch 13 Elapsed time 292.2694\n",
      "    svc combine loss 2 0.1444\n",
      "    total training loss 1.6159\n",
      "    extractor loss 1 1.1431\n",
      "    joint_embedding 0.3284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3201\n",
      "  CocoCapTest text2image_recall_top1 0.2642\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3911\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3487\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7994\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.904\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1428\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.4175\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.569\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1819\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4887\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6499\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0864\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2858\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4301\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2468\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.5789\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.746\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1123\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3364\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4769\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1417\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.393\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.538\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0626\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2297\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:06<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 36204 epoch 14 Elapsed time 289.3238\n",
      "    svc combine loss 2 0.1396\n",
      "    total training loss 1.5964\n",
      "    extractor loss 1 1.1357\n",
      "    joint_embedding 0.321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3453\n",
      "  CocoCapTest text2image_recall_top1 0.2724\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3509\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.2957\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7235\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.879\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1624\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.4506\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6064\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1965\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.517\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.665\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0887\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2654\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.386\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2455\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6043\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7573\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1136\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3237\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4627\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1436\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4214\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5694\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0664\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2079\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:04<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 38790 epoch 15 Elapsed time 286.9932\n",
      "    svc combine loss 2 0.1397\n",
      "    total training loss 1.5852\n",
      "    extractor loss 1 1.1359\n",
      "    joint_embedding 0.3097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3122\n",
      "  CocoCapTest text2image_recall_top1 0.2795\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2848\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.2889\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.6818\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.881\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1814\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.4716\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6157\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1446\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4146\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.5699\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0888\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.275\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.3942\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2583\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6306\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7771\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1405\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3791\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5139\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1221\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.3591\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5133\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.071\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2141\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:03<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 41376 epoch 16 Elapsed time 277.5809\n",
      "    svc combine loss 2 0.136\n",
      "    total training loss 1.5691\n",
      "    extractor loss 1 1.1262\n",
      "    joint_embedding 0.3069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3279\n",
      "  CocoCapTest text2image_recall_top1 0.2795\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2589\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3087\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.76\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.902\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2034\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5268\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6997\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.179\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4381\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.5959\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1093\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3375\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4771\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2474\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6293\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7867\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1466\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4025\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5544\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1454\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4037\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5553\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0835\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2554\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:08<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 43962 epoch 17 Elapsed time 281.2907\n",
      "    svc combine loss 2 0.1384\n",
      "    total training loss 1.5591\n",
      "    extractor loss 1 1.1113\n",
      "    joint_embedding 0.3094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3428\n",
      "  CocoCapTest text2image_recall_top1 0.2728\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2643\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.2485\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7188\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8495\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1513\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.4411\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6053\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.143\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.3957\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.5605\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0833\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2905\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4293\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2263\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6241\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7691\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1246\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.361\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5087\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1293\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.3619\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5049\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0672\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2268\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:03<00:00, 11.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 46548 epoch 18 Elapsed time 275.9551\n",
      "    svc combine loss 2 0.1267\n",
      "    total training loss 1.5318\n",
      "    extractor loss 1 1.1077\n",
      "    joint_embedding 0.2975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3445\n",
      "  CocoCapTest text2image_recall_top1 0.2824\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3812\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3319\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7766\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8647\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.1554\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.4227\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.5658\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1843\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4406\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.5779\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.0927\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.2723\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.3887\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2362\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6283\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7791\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1347\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.3545\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.4843\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1427\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4019\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.548\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0684\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2265\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:09<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 49134 epoch 19 Elapsed time 289.9097\n",
      "    svc combine loss 2 0.1304\n",
      "    total training loss 1.5369\n",
      "    extractor loss 1 1.1067\n",
      "    joint_embedding 0.2999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3369\n",
      "  CocoCapTest text2image_recall_top1 0.2964\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3125\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3053\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.738\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8793\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2075\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5274\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.672\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1831\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4473\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.596\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1074\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3181\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4387\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2468\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6321\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7736\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1448\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4011\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5442\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1585\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4056\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5486\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0809\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2414\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:02<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 51720 epoch 20 Elapsed time 285.1352\n",
      "    svc combine loss 2 0.1058\n",
      "    total training loss 1.389\n",
      "    extractor loss 1 1.0258\n",
      "    joint_embedding 0.2573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3647\n",
      "  CocoCapTest text2image_recall_top1 0.3099\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3027\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3597\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8068\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9022\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2318\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5393\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6844\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.2084\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.499\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6323\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1185\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3364\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4751\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2808\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6711\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8088\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1567\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4178\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5615\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1649\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4462\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5932\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0878\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.274\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:06<00:00, 10.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 54306 epoch 21 Elapsed time 287.8113\n",
      "    svc combine loss 2 0.0977\n",
      "    total training loss 1.3203\n",
      "    extractor loss 1 0.9798\n",
      "    joint_embedding 0.2428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3771\n",
      "  CocoCapTest text2image_recall_top1 0.3166\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3446\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3975\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7809\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8742\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2263\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5079\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6588\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.2073\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5051\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.642\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1282\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.35\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.486\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2851\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6654\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8053\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1604\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4118\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5498\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.166\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4398\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5851\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0908\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2762\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:05<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 56892 epoch 22 Elapsed time 288.6727\n",
      "    svc combine loss 2 0.0922\n",
      "    total training loss 1.301\n",
      "    extractor loss 1 0.9731\n",
      "    joint_embedding 0.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3732\n",
      "  CocoCapTest text2image_recall_top1 0.3178\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3116\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3304\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.755\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9027\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2297\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5214\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6737\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1947\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4626\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6084\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1109\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3298\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4745\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2873\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6514\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7915\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1621\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4228\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5522\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1573\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4161\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5562\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0835\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2657\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:03<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 59478 epoch 23 Elapsed time 286.2472\n",
      "    svc combine loss 2 0.0895\n",
      "    total training loss 1.2763\n",
      "    extractor loss 1 0.9664\n",
      "    joint_embedding 0.2204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3951\n",
      "  CocoCapTest text2image_recall_top1 0.3239\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3652\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3002\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7874\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8784\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2167\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5387\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.691\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1994\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.482\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6357\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1164\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3464\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4974\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2867\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6468\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7922\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1586\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4175\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.558\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.162\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4304\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5759\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0884\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2758\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:08<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 62064 epoch 24 Elapsed time 290.6112\n",
      "    svc combine loss 2 0.0858\n",
      "    total training loss 1.254\n",
      "    extractor loss 1 0.9464\n",
      "    joint_embedding 0.2218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.401\n",
      "  CocoCapTest text2image_recall_top1 0.3139\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3536\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.335\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.753\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8745\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2414\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5923\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7451\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.2004\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5024\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6377\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1216\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.358\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.5054\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2895\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6829\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8142\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1665\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4392\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5812\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.164\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4455\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5896\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0881\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2762\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:06<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 64650 epoch 25 Elapsed time 289.9528\n",
      "    svc combine loss 2 0.0851\n",
      "    total training loss 1.2465\n",
      "    extractor loss 1 0.9458\n",
      "    joint_embedding 0.2155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.3992\n",
      "  CocoCapTest text2image_recall_top1 0.3179\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3321\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3903\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7758\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9183\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2266\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5422\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7022\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1957\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5121\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6561\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1287\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3661\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.5096\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2873\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.666\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8057\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1623\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4386\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5854\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1685\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4572\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.6088\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0948\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2894\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.4099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:04<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 67236 epoch 26 Elapsed time 288.1647\n",
      "    svc combine loss 2 0.0846\n",
      "    total training loss 1.2391\n",
      "    extractor loss 1 0.94\n",
      "    joint_embedding 0.2146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4119\n",
      "  CocoCapTest text2image_recall_top1 0.32\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3429\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3591\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7806\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9185\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2194\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5413\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.207\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4999\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6426\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1275\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3643\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.5012\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.3\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6728\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8101\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1605\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4384\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5808\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1713\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4571\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.596\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0969\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2893\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:08<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 69822 epoch 27 Elapsed time 292.0722\n",
      "    svc combine loss 2 0.0816\n",
      "    total training loss 1.2301\n",
      "    extractor loss 1 0.9367\n",
      "    joint_embedding 0.2118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4006\n",
      "  CocoCapTest text2image_recall_top1 0.3385\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3473\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3391\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7449\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8943\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2595\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.609\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7519\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1889\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.45\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.593\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1237\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3525\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4935\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2939\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.67\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8115\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1743\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4498\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5961\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1582\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.431\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5675\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0895\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2855\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.4002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:05<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 72408 epoch 28 Elapsed time 289.2632\n",
      "    svc combine loss 2 0.0839\n",
      "    total training loss 1.2264\n",
      "    extractor loss 1 0.9307\n",
      "    joint_embedding 0.2117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4037\n",
      "  CocoCapTest text2image_recall_top1 0.3347\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.308\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3452\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8413\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9312\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2227\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5212\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6738\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1914\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4991\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6434\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1171\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3427\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4851\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2921\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6692\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.808\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1583\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4212\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5593\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1602\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4434\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5856\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0933\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.278\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:03<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 74994 epoch 29 Elapsed time 285.6285\n",
      "    svc combine loss 2 0.0802\n",
      "    total training loss 1.2105\n",
      "    extractor loss 1 0.924\n",
      "    joint_embedding 0.2063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4151\n",
      "  CocoCapTest text2image_recall_top1 0.3455\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3018\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.342\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7906\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9256\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2152\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5349\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6914\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1883\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5073\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6651\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1256\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3524\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4996\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2776\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6577\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8075\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1542\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4214\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5638\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1625\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4582\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.6049\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0962\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2818\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:06<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 77580 epoch 30 Elapsed time 288.6104\n",
      "    svc combine loss 2 0.0757\n",
      "    total training loss 1.1954\n",
      "    extractor loss 1 0.9202\n",
      "    joint_embedding 0.1995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4147\n",
      "  CocoCapTest text2image_recall_top1 0.3268\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3348\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3162\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8059\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9099\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2177\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5345\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.684\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.2076\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5115\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6653\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1216\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3481\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4849\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.3067\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6778\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8117\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1684\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4402\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5782\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1693\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4595\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.609\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0965\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2823\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.4004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:03<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 80166 epoch 31 Elapsed time 284.0558\n",
      "    svc combine loss 2 0.077\n",
      "    total training loss 1.191\n",
      "    extractor loss 1 0.9103\n",
      "    joint_embedding 0.2037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4259\n",
      "  CocoCapTest text2image_recall_top1 0.3329\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3402\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3711\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8035\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8967\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2482\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5644\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7126\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.225\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5172\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6696\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.131\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3604\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4984\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.3004\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6748\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8164\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1649\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4458\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5883\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.165\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4533\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5987\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.098\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2906\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.4109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:03<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 82752 epoch 32 Elapsed time 284.3683\n",
      "    svc combine loss 2 0.0762\n",
      "    total training loss 1.1823\n",
      "    extractor loss 1 0.9058\n",
      "    joint_embedding 0.2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4124\n",
      "  CocoCapTest text2image_recall_top1 0.3292\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3143\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3245\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8441\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9469\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2496\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5597\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7117\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1971\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5094\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6717\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1253\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.354\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4918\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2842\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6665\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.811\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1592\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4325\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5735\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1637\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4462\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5981\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0936\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2803\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.4064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:05<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 85338 epoch 33 Elapsed time 286.6426\n",
      "    svc combine loss 2 0.0729\n",
      "    total training loss 1.1733\n",
      "    extractor loss 1 0.9\n",
      "    joint_embedding 0.2004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4323\n",
      "  CocoCapTest text2image_recall_top1 0.3434\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3009\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.4254\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8483\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9065\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2408\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5762\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7356\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1907\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4977\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6448\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1216\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3386\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.481\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2711\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.657\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8073\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1588\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4377\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5815\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1554\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4385\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5929\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0874\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2724\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:07<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 87924 epoch 34 Elapsed time 288.76\n",
      "    svc combine loss 2 0.0737\n",
      "    total training loss 1.1687\n",
      "    extractor loss 1 0.9019\n",
      "    joint_embedding 0.1931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4139\n",
      "  CocoCapTest text2image_recall_top1 0.3301\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3464\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.4316\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8343\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9298\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2371\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5742\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7231\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.2105\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5038\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6717\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1341\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.367\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.5008\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2751\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6583\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8034\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1573\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4261\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5717\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1668\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4418\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.6046\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0987\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2805\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.4047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:07<00:00, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 90510 epoch 35 Elapsed time 289.0734\n",
      "    svc combine loss 2 0.0733\n",
      "    total training loss 1.1604\n",
      "    extractor loss 1 0.8968\n",
      "    joint_embedding 0.1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4189\n",
      "  CocoCapTest text2image_recall_top1 0.3481\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2723\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3476\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7698\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8878\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.235\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5687\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.715\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1986\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5054\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6598\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.126\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3499\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4892\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.27\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6556\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8053\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1559\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4303\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.572\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1642\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4613\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.6081\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0933\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2765\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.4013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:07<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 93096 epoch 36 Elapsed time 290.0877\n",
      "    svc combine loss 2 0.0724\n",
      "    total training loss 1.1553\n",
      "    extractor loss 1 0.8969\n",
      "    joint_embedding 0.186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4276\n",
      "  CocoCapTest text2image_recall_top1 0.3492\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2955\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.4086\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7846\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9209\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2245\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.545\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6939\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.2156\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5372\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6839\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1339\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3499\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4875\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2765\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6514\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8017\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1513\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4189\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5573\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1587\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.451\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.6072\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0936\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2785\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:05<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 95682 epoch 37 Elapsed time 288.8718\n",
      "    svc combine loss 2 0.0725\n",
      "    total training loss 1.1493\n",
      "    extractor loss 1 0.8889\n",
      "    joint_embedding 0.1879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4287\n",
      "  CocoCapTest text2image_recall_top1 0.3264\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3063\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3958\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7719\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9058\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2434\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5711\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7216\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1955\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4987\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6354\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1194\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.336\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4713\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2689\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6552\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7996\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1666\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4391\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5789\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1584\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4285\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5782\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0887\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2759\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:03<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 98268 epoch 38 Elapsed time 287.0614\n",
      "    svc combine loss 2 0.0724\n",
      "    total training loss 1.1551\n",
      "    extractor loss 1 0.893\n",
      "    joint_embedding 0.1898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4403\n",
      "  CocoCapTest text2image_recall_top1 0.3625\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3179\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.476\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8306\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.953\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2393\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5439\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6894\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.2146\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5387\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6971\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1286\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3526\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.488\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2814\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6392\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7832\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1562\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4125\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5501\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1617\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4387\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5929\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0956\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2824\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:04<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 100854 epoch 39 Elapsed time 287.7389\n",
      "    svc combine loss 2 0.0734\n",
      "    total training loss 1.1492\n",
      "    extractor loss 1 0.886\n",
      "    joint_embedding 0.1898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4369\n",
      "  CocoCapTest text2image_recall_top1 0.3441\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.4277\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.4101\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7753\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9324\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2402\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.542\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6911\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1946\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5069\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6605\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1241\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3399\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4765\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2783\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6308\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7742\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1621\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4186\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5557\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1564\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4407\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5923\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.096\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2805\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:04<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 103440 epoch 40 Elapsed time 286.9686\n",
      "    svc combine loss 2 0.0677\n",
      "    total training loss 1.1343\n",
      "    extractor loss 1 0.8777\n",
      "    joint_embedding 0.1888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4147\n",
      "  CocoCapTest text2image_recall_top1 0.3472\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3643\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3561\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8754\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.924\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2355\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5394\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6822\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1959\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4956\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6477\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1191\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3333\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4647\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2862\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6334\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7749\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1573\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4173\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5527\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1548\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4311\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5837\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0899\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2745\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:07<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 106026 epoch 41 Elapsed time 287.9341\n",
      "    svc combine loss 2 0.0679\n",
      "    total training loss 1.125\n",
      "    extractor loss 1 0.8751\n",
      "    joint_embedding 0.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4334\n",
      "  CocoCapTest text2image_recall_top1 0.3219\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3312\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3498\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.743\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9064\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2206\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5337\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6802\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.2069\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5198\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6704\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.118\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3365\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4803\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2915\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6517\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7923\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1616\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4223\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5607\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1609\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4385\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5924\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0944\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2746\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:02<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 108612 epoch 42 Elapsed time 284.3229\n",
      "    svc combine loss 2 0.0695\n",
      "    total training loss 1.1142\n",
      "    extractor loss 1 0.8632\n",
      "    joint_embedding 0.1815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.437\n",
      "  CocoCapTest text2image_recall_top1 0.3285\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.317\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.385\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8188\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9097\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2658\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5957\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7371\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.199\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4991\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6591\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1235\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3468\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.489\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2967\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6772\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8123\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1699\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4427\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5769\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1595\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4519\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.6009\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0968\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2821\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:04<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 111198 epoch 43 Elapsed time 284.9225\n",
      "    svc combine loss 2 0.066\n",
      "    total training loss 1.1171\n",
      "    extractor loss 1 0.869\n",
      "    joint_embedding 0.1821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4299\n",
      "  CocoCapTest text2image_recall_top1 0.346\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.4259\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.4028\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8414\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9324\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2448\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5431\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6858\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1952\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5101\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.663\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1269\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3478\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4857\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2908\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6533\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.789\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1609\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4208\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5582\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1591\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4453\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5943\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0965\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2863\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.4019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:09<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 113784 epoch 44 Elapsed time 290.3549\n",
      "    svc combine loss 2 0.0684\n",
      "    total training loss 1.1135\n",
      "    extractor loss 1 0.865\n",
      "    joint_embedding 0.1802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.441\n",
      "  CocoCapTest text2image_recall_top1 0.3518\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3366\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.4211\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8257\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9288\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2657\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5982\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7326\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.2131\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5074\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6611\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1268\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3439\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4819\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2988\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6736\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8161\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1704\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4403\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5824\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1623\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4482\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5958\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0929\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2764\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:07<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 116370 epoch 45 Elapsed time 290.1816\n",
      "    svc combine loss 2 0.066\n",
      "    total training loss 1.1206\n",
      "    extractor loss 1 0.8748\n",
      "    joint_embedding 0.1797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4256\n",
      "  CocoCapTest text2image_recall_top1 0.3548\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.2929\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3327\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8077\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9087\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2332\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5607\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.7062\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1987\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.5255\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6751\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1252\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3462\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4806\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2854\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6495\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7966\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1664\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4331\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5713\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1617\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4509\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.6037\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0932\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2798\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:07<00:00, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 118956 epoch 46 Elapsed time 291.0723\n",
      "    svc combine loss 2 0.071\n",
      "    total training loss 1.1252\n",
      "    extractor loss 1 0.8728\n",
      "    joint_embedding 0.1813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.446\n",
      "  CocoCapTest text2image_recall_top1 0.3431\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3652\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3541\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8097\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9113\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2466\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5539\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6933\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.2027\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4934\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6419\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1211\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3336\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4646\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2857\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6717\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8092\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1659\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4262\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5658\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1595\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4366\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5872\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0933\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2774\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:02<00:00, 10.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 121542 epoch 47 Elapsed time 286.0153\n",
      "    svc combine loss 2 0.066\n",
      "    total training loss 1.1153\n",
      "    extractor loss 1 0.8708\n",
      "    joint_embedding 0.1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4288\n",
      "  CocoCapTest text2image_recall_top1 0.3285\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3107\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.4468\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8336\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.9074\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2493\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5774\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.72\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1941\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4963\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6436\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1186\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3358\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4636\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2898\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6607\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8064\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.161\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4337\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5715\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.156\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4331\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5784\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0919\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2782\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:07<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 124128 epoch 48 Elapsed time 290.5747\n",
      "    svc combine loss 2 0.0675\n",
      "    total training loss 1.1184\n",
      "    extractor loss 1 0.8707\n",
      "    joint_embedding 0.1802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2586 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CocoCapTrain text2image_recall_top1 0.4281\n",
      "  CocoCapTest text2image_recall_top1 0.3399\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3152\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3874\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.7779\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8879\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2213\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.5099\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6492\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.1766\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.4697\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6244\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1199\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3343\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4653\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2733\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6261\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.7724\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1533\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.417\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5527\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1525\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4306\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5845\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0941\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2782\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.3961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [04:06<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 126714 epoch 49 Elapsed time 288.3528\n",
      "    svc combine loss 2 0.0654\n",
      "    total training loss 1.1191\n",
      "    extractor loss 1 0.8722\n",
      "    joint_embedding 0.1815\n",
      "  CocoCapTrain text2image_recall_top1 0.4247\n",
      "  CocoCapTest text2image_recall_top1 0.3495\n",
      "  SimpleImageCaptions112 text2image_recall_top1 0.3384\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top1 0.3533\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top5 0.8042\n",
      "  SimpleImageCaptions112 svc_ttt_recall_top10 0.8889\n",
      "  SimpleImageCaptions112 svc_tti_recall_top1 0.2411\n",
      "  SimpleImageCaptions112 svc_tti_recall_top5 0.533\n",
      "  SimpleImageCaptions112 svc_tti_recall_top10 0.6779\n",
      "  SimpleImageCaptions112 svc_tit_recall_top1 0.2074\n",
      "  SimpleImageCaptions112 svc_tit_recall_top5 0.506\n",
      "  SimpleImageCaptions112 svc_tit_recall_top10 0.6578\n",
      "  SimpleImageCaptions112 svc_tii_recall_top1 0.1266\n",
      "  SimpleImageCaptions112 svc_tii_recall_top5 0.3381\n",
      "  SimpleImageCaptions112 svc_tii_recall_top10 0.4731\n",
      "  SimpleImageCaptions112 svc_itt_recall_top1 0.2846\n",
      "  SimpleImageCaptions112 svc_itt_recall_top5 0.6652\n",
      "  SimpleImageCaptions112 svc_itt_recall_top10 0.8032\n",
      "  SimpleImageCaptions112 svc_iti_recall_top1 0.1639\n",
      "  SimpleImageCaptions112 svc_iti_recall_top5 0.4179\n",
      "  SimpleImageCaptions112 svc_iti_recall_top10 0.5616\n",
      "  SimpleImageCaptions112 svc_iit_recall_top1 0.1589\n",
      "  SimpleImageCaptions112 svc_iit_recall_top5 0.4375\n",
      "  SimpleImageCaptions112 svc_iit_recall_top10 0.5914\n",
      "  SimpleImageCaptions112 svc_iii_recall_top1 0.0925\n",
      "  SimpleImageCaptions112 svc_iii_recall_top5 0.2769\n",
      "  SimpleImageCaptions112 svc_iii_recall_top10 0.391\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\n",
    "    ##########################################\n",
    "    print 'It', it, 'epoch', epoch, 'Elapsed time', round(time.time() - tic, 4)\n",
    "    tic = time.time()\n",
    "    epoch += 1\n",
    "    for loss_name in losses_tracking:\n",
    "        avg_loss = np.mean(losses_tracking[loss_name][-999:])\n",
    "        print '   ', loss_name, round(avg_loss, 4)\n",
    "        logger.add_scalar(loss_name, avg_loss, it)\n",
    "        \n",
    "    if True:\n",
    "        tests = []\n",
    "        for dataset in [trainset, testset, sic112]:\n",
    "            t = test(dataset)\n",
    "            tests += [(dataset.name() + ' ' + metric_name, metric_value) for metric_name, metric_value in t]\n",
    "\n",
    "        for metric_name, metric_value in tests:\n",
    "            print ' ', metric_name, round(metric_value, 4)\n",
    "            logger.add_scalar(metric_name, metric_value, epoch)\n",
    "    if epoch >= opt.num_epochs:\n",
    "        break\n",
    "\n",
    "    ##########################################\n",
    "    model.train()\n",
    "    loader = trainset.get_loader(\n",
    "        batch_size=opt.batch_size, shuffle=True,\n",
    "        drop_last=True, num_workers=opt.loader_num_workers)\n",
    "    for data in tqdm(loader):\n",
    "        it += 1\n",
    "        losses = []\n",
    "\n",
    "        # learing rate scheduling\n",
    "        if it >= opt.learning_rate_decay_frequency and it % opt.learning_rate_decay_frequency == 0:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] *= 0.1\n",
    "\n",
    "        # joint embedding\n",
    "        imgs = np.stack([d['image'] for d in data])\n",
    "        imgs = torch.from_numpy(imgs).float()\n",
    "        if len(imgs.shape) == 2:\n",
    "            imgs = model.img_encoder.fc(imgs.cuda())\n",
    "        else:\n",
    "            imgs = model.img_encoder(imgs.cuda())\n",
    "        texts = [random.choice(d['captions']) for d in data]\n",
    "        texts = model.text_encoder(texts)\n",
    "        loss_name = 'joint_embedding'\n",
    "        loss_weight = 1.0\n",
    "        loss_value = pair_loss(texts, imgs).cuda()\n",
    "        losses += [(loss_name, loss_weight, loss_value)]\n",
    "        \n",
    "        \n",
    "        if epoch >= 2:\n",
    "            \n",
    "            def extractor_loss(from_image = False):\n",
    "                subjects = [trainset.imgs[d['index']]['subjects'] for d in data]\n",
    "                verbs = [trainset.imgs[d['index']]['verbs'] for d in data]\n",
    "                contexts = [trainset.imgs[d['index']]['contexts'] for d in data]\n",
    "\n",
    "                if from_image:\n",
    "                    a = torch.cat([imgs[i:(i+1),:] for i in range(texts.shape[0]) if len(subjects[i]) > 0])\n",
    "                    b = torch.cat([imgs[i:(i+1),:] for i in range(texts.shape[0]) if len(verbs[i]) > 0])\n",
    "                    c = torch.cat([imgs[i:(i+1),:] for i in range(texts.shape[0]) if len(contexts[i]) > 0])\n",
    "                else:\n",
    "                    a = torch.cat([texts[i:(i+1),:] for i in range(texts.shape[0]) if len(subjects[i]) > 0])\n",
    "                    b = torch.cat([texts[i:(i+1),:] for i in range(texts.shape[0]) if len(verbs[i]) > 0])\n",
    "                    c = torch.cat([texts[i:(i+1),:] for i in range(texts.shape[0]) if len(contexts[i]) > 0])\n",
    "\n",
    "                extracted_subjects = model.subject_extractor(a)\n",
    "                extracted_verbs = model.verb_extractor(b)\n",
    "                extracted_contexts = model.context_extractor(c)\n",
    "\n",
    "                subjects = [np.random.choice(i) for i in subjects if len(i) > 0]\n",
    "                verbs = [np.random.choice(i) for i in verbs if len(i) > 0]\n",
    "                contexts = [np.random.choice(i) for i in contexts if len(i) > 0]\n",
    "\n",
    "                encoded_subjects = model.text_encoder([str(i) for i in subjects])\n",
    "                encoded_verbs = model.text_encoder([str(i) for i in verbs])\n",
    "                encoded_contexts = model.text_encoder([str(i) for i in contexts])\n",
    "\n",
    "                return pair_loss(\n",
    "                    torch.cat([extracted_subjects, extracted_verbs, extracted_contexts]),\n",
    "                    torch.cat([encoded_subjects, encoded_verbs, encoded_contexts])\n",
    "                )\n",
    "            \n",
    "            def combine_loss(img_target = True):\n",
    "                subjects = [trainset.imgs[d['index']]['subjects'] for d in data]\n",
    "                verbs = [trainset.imgs[d['index']]['verbs'] for d in data]\n",
    "                contexts = [trainset.imgs[d['index']]['contexts'] for d in data]\n",
    "                iii = [i for i, d in enumerate(data) if len(trainset.imgs[d['index']]['verbs']) > 0 and \n",
    "                                                       len(trainset.imgs[d['index']]['contexts']) > 0 and \n",
    "                                                       len(trainset.imgs[d['index']]['subjects']) > 0 ]\n",
    "\n",
    "                subjects = [subjects[i] for i in iii]\n",
    "                verbs = [verbs[i] for i in iii]\n",
    "                contexts = [contexts[i] for i in iii]\n",
    "                \n",
    "                subjects = [np.random.choice(i) for i in subjects]\n",
    "                verbs = [np.random.choice(i) for i in verbs]\n",
    "                contexts = [np.random.choice(i) for i in contexts]\n",
    "                if img_target:\n",
    "                    svc_img = imgs[iii,:]\n",
    "                else:\n",
    "                    svc_img =  model.text_encoder([' '.join([s, v, c]) for s,v,c in zip(subjects, verbs, contexts)])\n",
    "                \n",
    "                subjects = model.text_encoder([str(i) for i in subjects])\n",
    "                verbs = model.text_encoder([str(i) for i in verbs])\n",
    "                contexts = model.text_encoder([str(i) for i in contexts])\n",
    "                \n",
    "                svc_combine = model.svc_combine([subjects, verbs, contexts])\n",
    "                return pair_loss(svc_img, svc_combine)\n",
    "            \n",
    "            def extract_combine_loss(reverse = False):\n",
    "                subjects = [trainset.imgs[d['index']]['subjects'] for d in data]\n",
    "                verbs = [trainset.imgs[d['index']]['verbs'] for d in data]\n",
    "                contexts = [trainset.imgs[d['index']]['contexts'] for d in data]\n",
    "\n",
    "                a = torch.cat([imgs[i:(i+1),:] for i in range(texts.shape[0]) if len(subjects[i]) > 0])\n",
    "                b = torch.cat([imgs[i:(i+1),:] for i in range(texts.shape[0]) if len(verbs[i]) > 0])\n",
    "                c = torch.cat([imgs[i:(i+1),:] for i in range(texts.shape[0]) if len(contexts[i]) > 0])\n",
    "\n",
    "                extracted_subjects = model.subject_extractor(a)\n",
    "                extracted_verbs = model.verb_extractor(b)\n",
    "                extracted_contexts = model.context_extractor(c)\n",
    "\n",
    "                subjects = [np.random.choice(i) for i in subjects if len(i) > 0]\n",
    "                verbs = [np.random.choice(i) for i in verbs if len(i) > 0]\n",
    "                contexts = [np.random.choice(i) for i in contexts if len(i) > 0]\n",
    "                \n",
    "                n = min(len(subjects), len(verbs), len(contexts))\n",
    "                iii = np.random.permutation(n)\n",
    "                extracted_subjects = extracted_subjects[iii,:]\n",
    "                extracted_verbs = extracted_verbs[iii,:]\n",
    "                extracted_contexts = extracted_contexts[iii,:]\n",
    "                subjects = [subjects[i] for i in iii]\n",
    "                verbs = [verbs[i] for i in iii]\n",
    "                contexts = [contexts[i] for i in iii]\n",
    "                \n",
    "                if reverse:\n",
    "                    svc =  model.text_encoder([' '.join([s, v, c]) for s,v,c in zip(subjects, verbs, contexts)])\n",
    "                    \n",
    "                    return pair_loss(\n",
    "                        torch.cat([extracted_subjects, extracted_verbs, extracted_contexts]),\n",
    "                        torch.cat([\n",
    "                            model.subject_extractor(svc),\n",
    "                            model.verb_extractor(svc),\n",
    "                            model.context_extractor(svc)\n",
    "                        ])\n",
    "                    )\n",
    "                \n",
    "                else:\n",
    "                    svc =  model.text_encoder([' '.join([s, v, c]) for s,v,c in zip(subjects, verbs, contexts)])\n",
    "                    combine_svc = model.svc_combine([extracted_subjects, extracted_verbs, extracted_contexts])\n",
    "                    return pair_loss(svc, combine_svc)\n",
    "            \n",
    "            \n",
    "            # extract part, text only\n",
    "            if 1:\n",
    "                    loss_value = extractor_loss(from_image = True) # / 2 + extractor_loss(from_image = False) / 2\n",
    "                    losses += [('extractor loss 1', 1.0, loss_value)]\n",
    "                    \n",
    "            # combine2\n",
    "            if 1:\n",
    "                loss_value = combine_loss(img_target = True)  / 2 + combine_loss(img_target = False) / 2 \n",
    "                losses += [('svc combine loss 2', 1.0, loss_value)]\n",
    "                \n",
    "                \n",
    "            #loss_value = extract_combine_loss(reverse = True)\n",
    "            #losses += [('extractor combine loss', 0.5, loss_value)]\n",
    "            \n",
    "        # total loss\n",
    "        total_loss = sum([loss_weight * loss_value for loss_name, loss_weight, loss_value in losses])\n",
    "        assert(not torch.isnan(total_loss))\n",
    "        losses += [('total training loss', None, total_loss)]\n",
    "        # save losses\n",
    "        for loss_name, loss_weight, loss_value in losses:\n",
    "            if not losses_tracking.has_key(loss_name):\n",
    "                losses_tracking[loss_name] = []\n",
    "            losses_tracking[loss_name].append(loss_value.data.item())\n",
    "        # backward & step\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fa3f6d411be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset.imgs[52234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip([1,2,3], [4, 5, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.permutation(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "img_features = []\n",
    "img_labels = []\n",
    "text_features = []\n",
    "text_labels = []\n",
    "for data in testset.get_loader(batch_size = opt.batch_size, shuffle = False, drop_last= False):\n",
    "    # extract image features\n",
    "    imgs = np.stack([d['image'] for d in data])\n",
    "    imgs = torch.from_numpy(imgs).float()\n",
    "    if len(imgs.shape) == 2:\n",
    "        imgs = model.img_encoder.fc(imgs.cuda())\n",
    "    else:\n",
    "        imgs = model.img_encoder(imgs.cuda())\n",
    "    imgs = model.snorm(imgs).cpu().detach().numpy()\n",
    "    img_features += [imgs]\n",
    "    img_labels += [d['label'] for d in data]\n",
    "\n",
    "    # text\n",
    "    texts = []\n",
    "    for d in data:\n",
    "        texts += d['captions']\n",
    "        text_labels += [d['label'] for c in d['captions']]\n",
    "    texts = model.text_encoder(texts)\n",
    "    texts = model.snorm(texts).cpu().detach().numpy()\n",
    "    text_features += [texts]\n",
    "\n",
    "img_features = np.concatenate(img_features, axis=0)\n",
    "text_features = np.concatenate(text_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_test_img(i):\n",
    "    f = testset.imgs[i]['filename']\n",
    "    plt.imshow(torchvision.datasets.folder.pil_loader(f))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(testset.imgs))\n",
    "show_test_img(i)\n",
    "\n",
    "j = np.random.randint(0, len(testset.imgs))\n",
    "show_test_img(j)\n",
    "\n",
    "k = np.random.randint(0, len(testset.imgs))\n",
    "show_test_img(k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = img_features[i:(i+1),:]\n",
    "x = model.subject_extractor(torch.from_numpy(x).cuda())\n",
    "# x = model.text_encoder(['a woman'])\n",
    "\n",
    "y = img_features[j:(j+1),:]\n",
    "y = model.verb_extractor(torch.from_numpy(y).cuda())\n",
    "#y = model.text_encoder(['playing tennis'])\n",
    "\n",
    "z = img_features[k:(k+1),:]\n",
    "z = model.context_extractor(torch.from_numpy(z).cuda())\n",
    "z = model.text_encoder(['on the beach'])\n",
    "\n",
    "x = model.svc_combine([x,y,z])\n",
    "x = x.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sim = x.dot(img_features.T)\n",
    "s = -sim[0,:]\n",
    "s = np.argsort(s)\n",
    "for h in s[:10]:\n",
    "    show_test_img(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
